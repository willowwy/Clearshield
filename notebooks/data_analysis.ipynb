{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data Analysis\n",
    "\n",
    "This notebook contains various data analysis and exploration tools for the ClearShield training pipeline.\n",
    "\n",
    "**Purpose**: Analyze and validate training data at different pipeline stages\n",
    "\n",
    "**Sections**:\n",
    "1. Cleaned Data Analysis\n",
    "2. Member Transaction Distribution\n",
    "3. Fraud Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config.pipeline_config import get_train_config\n",
    "\n",
    "# Get configuration\n",
    "config = get_train_config()\n",
    "\n",
    "print(f\"Project Root: {config.PROJECT_ROOT}\")\n",
    "print(f\"Data Root: {config.DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Cleaned Data Analysis\n",
    "\n",
    "Analyze the cleaned data files after Stage 1 preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 File Overview: Size, Rows, and Date Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = str(config.get_path('cleaned'))\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "print(f\"Analyzing {len(csv_files)} file(s) in: {CLEANED_DIR}\\n\")\n",
    "\n",
    "# Load all dataframes\n",
    "dfs = {}\n",
    "for filename in csv_files:\n",
    "    dfs[filename] = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "\n",
    "# Collect stats\n",
    "stats = []\n",
    "for filename, df in dfs.items():\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    min_date = df['Post Date'].min()\n",
    "    max_date = df['Post Date'].max()\n",
    "    fraud_count = ((df['Fraud Adjustment Indicator'].notna()) &\n",
    "                   (df['Fraud Adjustment Indicator'] != '')).sum()\n",
    "\n",
    "    stats.append({\n",
    "        'File': filename,\n",
    "        'Rows': len(df),\n",
    "        'Members': df['Member ID'].nunique(),\n",
    "        'Date From': min_date.strftime('%m/%d/%Y') if pd.notna(min_date) else 'N/A',\n",
    "        'Date To': max_date.strftime('%m/%d/%Y') if pd.notna(max_date) else 'N/A',\n",
    "        'Days': (max_date - min_date).days if pd.notna(min_date) else 0,\n",
    "        'Fraud %': round(fraud_count / len(df) * 100, 4) if len(df) > 0 else 0\n",
    "    })\n",
    "\n",
    "# Display table\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "# Summary\n",
    "all_members = set()\n",
    "for df in dfs.values():\n",
    "    all_members.update(df['Member ID'].dropna())\n",
    "\n",
    "total_fraud = sum([((dfs[f]['Fraud Adjustment Indicator'].notna()) &\n",
    "                    (dfs[f]['Fraud Adjustment Indicator'] != '')).sum()\n",
    "                   for f in dfs.keys()])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total Rows: {df_stats['Rows'].sum():,}\")\n",
    "print(f\"Total Unique Members: {len(all_members):,}\")\n",
    "print(f\"Total Fraud Indicators: {total_fraud:,}\")\n",
    "print(f\"Overall Fraud %: {round(total_fraud / df_stats['Rows'].sum() * 100, 4)}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 File Overlap Detection\n",
    "\n",
    "Check for duplicate records across different CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = str(config.get_path('cleaned'))\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "print(f\"Loading {len(csv_files)} file(s) for overlap detection...\\n\")\n",
    "\n",
    "# Load files and create row IDs\n",
    "file_rows = {}\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "    row_ids = set(df['Account ID'].astype(str) + '|' +\n",
    "                  df['Member ID'].astype(str) + '|' +\n",
    "                  df['Post Date'].astype(str) + '|' +\n",
    "                  df['Post Time'].astype(str) + '|' +\n",
    "                  df['Amount'].astype(str))\n",
    "    file_rows[filename] = row_ids\n",
    "\n",
    "# Calculate pairwise overlaps\n",
    "results = []\n",
    "for i, file1 in enumerate(csv_files):\n",
    "    for j, file2 in enumerate(csv_files):\n",
    "        if i < j:\n",
    "            overlap = len(file_rows[file1] & file_rows[file2])\n",
    "            pct1 = overlap / len(file_rows[file1]) * 100\n",
    "            pct2 = overlap / len(file_rows[file2]) * 100\n",
    "\n",
    "            results.append({\n",
    "                'File 1': file1,\n",
    "                'File 2': file2,\n",
    "                'Overlap Rows': overlap,\n",
    "                '% of File 1': round(pct1, 2),\n",
    "                '% of File 2': round(pct2, 2)\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if df_results.empty:\n",
    "    print(\"✓ Only one file found or no overlaps detected\")\n",
    "else:\n",
    "    print(\"Overlap Analysis Results:\")\n",
    "    display(df_results)\n",
    "    \n",
    "    # Highlight significant overlaps\n",
    "    significant = df_results[df_results['Overlap Rows'] > 0]\n",
    "    if not significant.empty:\n",
    "        print(f\"\\n⚠ Warning: Found {len(significant)} file pair(s) with overlapping records\")\n",
    "    else:\n",
    "        print(\"\\n✓ No overlapping records found between files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Member Transaction Distribution\n",
    "\n",
    "Analyze transaction count distribution across members after Stage 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BY_MEMBER_DIR = str(config.get_path('by_member_temp'))\n",
    "threshold = config.fraud_matching['min_history_length']\n",
    "\n",
    "print(f\"Analyzing member files in: {BY_MEMBER_DIR}\")\n",
    "print(f\"Minimum history threshold: {threshold}\\n\")\n",
    "\n",
    "# Get all member files and count transactions\n",
    "member_files = glob(os.path.join(BY_MEMBER_DIR, 'member_*.csv'))\n",
    "\n",
    "if not member_files:\n",
    "    print(\"⚠ No member files found. Please run Stage 3 first.\")\n",
    "else:\n",
    "    counts = [len(pd.read_csv(f)) for f in member_files]\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_count = len(counts)\n",
    "    above_n = sum(1 for c in counts if c >= threshold)\n",
    "    below_n = total_count - above_n\n",
    "    above_ratio = (above_n / total_count) * 100\n",
    "    below_ratio = (below_n / total_count) * 100\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"MEMBER TRANSACTION DISTRIBUTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Threshold: {threshold} transactions\")\n",
    "    print(f\"Total Members: {total_count:,}\")\n",
    "    print(f\"\\nMembers >= {threshold} txns: {above_n:,} ({above_ratio:.2f}%)\")\n",
    "    print(f\"Members < {threshold} txns:  {below_n:,} ({below_ratio:.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Distribution visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Histogram\n",
    "    axes[0].hist(counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n",
    "    axes[0].set_xlabel('Number of Transactions')\n",
    "    axes[0].set_ylabel('Number of Members')\n",
    "    axes[0].set_title('Transaction Count Distribution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Box plot\n",
    "    axes[1].boxplot(counts, vert=True)\n",
    "    axes[1].axhline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n",
    "    axes[1].set_ylabel('Number of Transactions')\n",
    "    axes[1].set_title('Transaction Count Box Plot')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Additional statistics\n",
    "    print(f\"\\nDistribution Statistics:\")\n",
    "    print(f\"  Mean: {sum(counts) / len(counts):.2f}\")\n",
    "    print(f\"  Median: {sorted(counts)[len(counts)//2]}\")\n",
    "    print(f\"  Min: {min(counts)}\")\n",
    "    print(f\"  Max: {max(counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Fraud Statistics Analysis\n",
    "\n",
    "Analyze fraud matching results after Stage 3 completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read member summary\n",
    "summary_path = config.get_path('by_member') / 'member_summary.csv'\n",
    "\n",
    "if summary_path.exists():\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"FRAUD MATCHING RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total Members Processed: {len(summary_df):,}\")\n",
    "    print(f\"\\nCategory Breakdown:\")\n",
    "    print(summary_df['Category'].value_counts())\n",
    "    \n",
    "    # Detailed statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FRAUD ADJUSTMENT STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for category in ['matched', 'unmatched', 'no_fraud']:\n",
    "        cat_df = summary_df[summary_df['Category'] == category]\n",
    "        if not cat_df.empty:\n",
    "            print(f\"\\n{category.upper()}:\")\n",
    "            print(f\"  Members: {len(cat_df):,}\")\n",
    "            print(f\"  Total Transactions: {cat_df['Total_Transactions'].sum():,}\")\n",
    "            print(f\"  Fraud Adjustments: {cat_df['Fraud_Adjustments'].sum():,}\")\n",
    "            print(f\"  Matched: {cat_df['Matched'].sum():,}\")\n",
    "    \n",
    "    # Visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Category distribution\n",
    "    category_counts = summary_df['Category'].value_counts()\n",
    "    axes[0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0].set_title('Member Distribution by Category')\n",
    "    \n",
    "    # Match rate\n",
    "    match_data = summary_df[summary_df['Fraud_Adjustments'] > 0]\n",
    "    if not match_data.empty:\n",
    "        match_data['Match_Rate'] = match_data['Matched'] / match_data['Fraud_Adjustments'] * 100\n",
    "        axes[1].hist(match_data['Match_Rate'], bins=20, edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_xlabel('Match Rate (%)')\n",
    "        axes[1].set_ylabel('Number of Members')\n",
    "        axes[1].set_title('Fraud Adjustment Match Rate Distribution')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ Member summary not found. Please run Stage 3 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Column Statistics\n",
    "\n",
    "Analyze column distributions and data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_DIR = str(config.get_path('cleaned'))\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "if csv_files:\n",
    "    # Load first file for analysis\n",
    "    df = pd.read_csv(os.path.join(CLEANED_DIR, csv_files[0]))\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"DATA QUALITY ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nAnalyzing: {csv_files[0]}\")\n",
    "    print(f\"Total Rows: {len(df):,}\\n\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': missing.index,\n",
    "        'Missing': missing.values,\n",
    "        'Percent': missing_pct.values\n",
    "    })\n",
    "    display(missing_df[missing_df['Missing'] > 0])\n",
    "    \n",
    "    # Categorical columns\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CATEGORICAL COLUMNS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    categorical_cols = ['Account Type', 'Action Type', 'Source Type']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(df[col].value_counts().head(10))\n",
    "    \n",
    "    # Numerical columns\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"NUMERICAL STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(df['Amount'].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No cleaned files found. Please run Stage 1 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides comprehensive analysis tools for the ClearShield training data pipeline. \n",
    "\n",
    "Use these analyses to:\n",
    "- Validate data quality\n",
    "- Understand data distributions\n",
    "- Monitor fraud matching performance\n",
    "- Detect potential issues early"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
