{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Pipeline for Data Preprocessing\n",
    "\n",
    "Complete 4-stage data preprocessing pipeline for model prediction/inference.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "1. Data Cleaning: Raw CSV → Cleaned CSV\n",
    "2. Feature Engineering (Inference): Cleaned CSV → Clustered CSV (using pre-trained model)\n",
    "3. Reorganize by Member: Clustered CSV → Organized by Member (Fraud=0)\n",
    "4. Feature Encoding: Processed CSV → Final Encoded Dataset\n",
    "\n",
    "**Key Differences from Training:**\n",
    "- Stage 2: Uses pre-trained clustering model (no training)\n",
    "- Stage 3: Simple reorganization with Fraud=0 (no fraud matching)\n",
    "- No temp directory needed\n",
    "- Simpler output structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import and Configuration\n",
    "\n",
    "**New Approach**: All paths and parameters are configured in `config/pipeline_config.py`\n",
    "- No more scattered path definitions\n",
    "- Easy to switch between train/pred modes\n",
    "- Centralized parameter management"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:37:49.507087Z",
     "start_time": "2025-11-20T05:37:49.501339Z"
    }
   },
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import centralized configuration\n",
    "from config.pipeline_config import get_pred_config\n",
    "\n",
    "# Get prediction configuration\n",
    "config = get_pred_config()\n",
    "config.print_config()\n",
    "\n",
    "# Create all necessary directories\n",
    "print(\"\\nCreating directories...\")\n",
    "config.create_directories()\n",
    "\n",
    "print(\"\\n✓ Configuration loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ClearShield Pipeline Configuration - Mode: PRED\n",
      "======================================================================\n",
      "\n",
      "Project Root: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield\n",
      "\n",
      "Data Paths:\n",
      "  raw                 : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/raw\n",
      "  cleaned             : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/cleaned\n",
      "  clustered           : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/clustered_out\n",
      "  by_member           : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/by_member\n",
      "  final               : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/final\n",
      "  model_dir           : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/02_feature_engineering/02b_description_encoding\n",
      "  cluster_model       : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/02_feature_engineering/02b_description_encoding/global_cluster_model.pkl\n",
      "  tokenize_config     : /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/config/tokenize_dict.json\n",
      "\n",
      "Feature Engineering Parameters:\n",
      "  model_name          : prajjwal1/bert-tiny\n",
      "  text_column         : Transaction Description\n",
      "  batch_size          : 64\n",
      "  max_length          : 64\n",
      "  pca_dim             : 20\n",
      "  min_k               : 10\n",
      "  max_k               : 60\n",
      "  k_step              : 10\n",
      "  sample_size         : 10000\n",
      "  cluster_batch_size  : 4096\n",
      "  random_state        : 42\n",
      "======================================================================\n",
      "\n",
      "Creating directories...\n",
      "Created 4 directories:\n",
      "  ✓ /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/cleaned\n",
      "  ✓ /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/clustered_out\n",
      "  ✓ /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/by_member\n",
      "  ✓ /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/final\n",
      "\n",
      "✓ Configuration loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processing Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:37:57.916569Z",
     "start_time": "2025-11-20T05:37:55.016880Z"
    }
   },
   "source": [
    "# Import data cleaning module\n",
    "sys.path.insert(0, os.path.abspath('./01_data_cleaning'))\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"data_cleaning\",\n",
    "    \"./01_data_cleaning/01_data_cleaning.py\"\n",
    ")\n",
    "dc = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(dc)\n",
    "\n",
    "# Import feature engineering inference module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"inference_stage2\",\n",
    "    \"02_feature_engineering/inference_stage2.py\"\n",
    ")\n",
    "fe_inf = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(fe_inf)\n",
    "\n",
    "# Import fraud relabeling inference module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"inference_stage3\",\n",
    "    \"03_fraud_relabeling/inference_stage3.py\"\n",
    ")\n",
    "fr_inf = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(fr_inf)\n",
    "\n",
    "# Import encoding inference module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"inference_stage4\",\n",
    "    \"04_encoding/inference_stage4.py\"\n",
    ")\n",
    "enc_inf = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(enc_inf)\n",
    "\n",
    "print(\"✓ All modules loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Modules Using Centralized Config"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:38:02.857987Z",
     "start_time": "2025-11-20T05:38:02.852728Z"
    }
   },
   "source": [
    "# Configure data cleaning\n",
    "dc.ENABLE_RENAMING = True\n",
    "dc.RAW_DIR = str(config.get_path('raw'))\n",
    "dc.CLEANED_DIR = str(config.get_path('cleaned'))\n",
    "\n",
    "# Get stage I/O paths\n",
    "stage2_io = config.get_stage_io(2)\n",
    "stage3_io = config.get_stage_io(3)\n",
    "stage4_io = config.get_stage_io(4)\n",
    "\n",
    "# Configure inference stage 3\n",
    "fr_inf.INPUT_DIR = str(stage3_io['input'])\n",
    "fr_inf.OUTPUT_MEMBER_DIR = str(stage3_io['output'])\n",
    "\n",
    "# Configure inference stage 4\n",
    "enc_inf.PROCESSED_DIR = str(stage4_io['input'])\n",
    "enc_inf.OUTPUT_DIR = str(stage4_io['output'])\n",
    "enc_inf.CONFIG_PATH = str(config.get_path('tokenize_config'))\n",
    "\n",
    "# Model paths\n",
    "cluster_model_path = str(config.get_path('cluster_model'))\n",
    "tokenize_config_path = str(config.get_path('tokenize_config'))\n",
    "\n",
    "print(\"✓ All modules configured successfully!\")\n",
    "print(f\"\\nPipeline will process:\")\n",
    "print(f\"  Raw data:     {dc.RAW_DIR}\")\n",
    "print(f\"  Final output: {enc_inf.OUTPUT_DIR}\")\n",
    "print(f\"\\nUsing pre-trained models:\")\n",
    "print(f\"  Cluster model: {cluster_model_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules configured successfully!\n",
      "\n",
      "Pipeline will process:\n",
      "  Raw data:     /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/raw\n",
      "  Final output: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/final\n",
      "\n",
      "Using pre-trained models:\n",
      "  Cluster model: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/02_feature_engineering/02b_description_encoding/global_cluster_model.pkl\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Cleaning\n",
    "\n",
    "This cell performs the following preprocessing tasks:\n",
    "1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n",
    "2. Fix comma issues (remove extra commas in field values)\n",
    "3. Clean Amount field (remove $ and commas, convert to numeric)\n",
    "4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n",
    "5. Rename files based on date range (MM-DD-YYYY_to_MM-DD-YYYY.csv)\n",
    "\n",
    "**Data Flow**: `data/pred/raw/` → `data/pred/cleaned/`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:38:06.291201Z",
     "start_time": "2025-11-20T05:38:06.073790Z"
    }
   },
   "source": [
    "dc.main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/raw\n",
      "Cleaned: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/cleaned\n",
      "\n",
      "Found 1 CSV files in /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. test.csv (0.06 MB)\n",
      "\n",
      "============================================================\n",
      "Processing Files...\n",
      "============================================================\n",
      "\n",
      "[1/1] test.csv... Missing:31, →01-01-2025_to_01-01-2025.csv\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Engineering (Inference Mode)\n",
    "\n",
    "This stage applies the pre-trained clustering model to new data:\n",
    "\n",
    "1. **BERT Encoding**: Use the same BERT-tiny model to encode \"Transaction Description\"\n",
    "2. **Apply Pre-trained PCA**: Transform using saved PCA model\n",
    "3. **Apply Pre-trained Clustering**: Assign cluster IDs using saved KMeans model\n",
    "4. **Add Cluster ID**: Append `cluster_id` column to each CSV file\n",
    "\n",
    "**Data Flow**: `data/pred/cleaned/` → `data/pred/clustered_out/`\n",
    "\n",
    "**Note**: This uses the pre-trained model saved during training. No model training occurs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:38:12.385501Z",
     "start_time": "2025-11-20T05:38:10.136127Z"
    }
   },
   "source": [
    "# Check if model exists\n",
    "if not os.path.exists(cluster_model_path):\n",
    "    raise FileNotFoundError(f\"Pre-trained model not found: {cluster_model_path}\")\n",
    "\n",
    "# Apply clustering using pre-trained model\n",
    "output_files = fe_inf.infer_clusters_for_directory(\n",
    "    input_dir=str(stage2_io['input']),\n",
    "    output_dir=str(stage2_io['output']),\n",
    "    model_path=cluster_model_path,\n",
    "    text_column=config.feature_engineering['text_column'],\n",
    "    batch_size=config.feature_engineering['batch_size'],\n",
    "    verbose=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] Found 1 CSV files\n",
      "\n",
      "[1/1] Processing: 01-01-2025_to_01-01-2025.csv\n",
      "[Load Model] /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/02_feature_engineering/02b_description_encoding/global_cluster_model.pkl\n",
      "  BERT Model: prajjwal1/bert-tiny\n",
      "  PCA Components: 20\n",
      "  KMeans Clusters: 60\n",
      "[Process] /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/cleaned/01-01-2025_to_01-01-2025.csv\n",
      "  Rows: 537\n",
      "  Device: mps\n",
      "  Encoding: 537/537 (100.0%)\r\n",
      "  Applying PCA...\n",
      "  Assigning clusters...\n",
      "[Done] Saved → /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/clustered_out/01-01-2025_to_01-01-2025.csv\n",
      "  Cluster distribution: min=0, max=59, unique=55\n",
      "\n",
      "[Complete] Processed 1/1 files\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Reorganize by Member (Inference Mode)\n",
    "\n",
    "This stage reorganizes transactions by member and sets Fraud=0 for all transactions:\n",
    "\n",
    "1. **Group by Member**: Reorganize transactions by Member ID\n",
    "2. **Set Fraud=0**: All transactions are assumed non-fraudulent\n",
    "3. **Save by Member**: Create individual member files\n",
    "\n",
    "**Data Flow**: `data/pred/clustered_out/` → `data/pred/by_member/`\n",
    "\n",
    "**Note**: No fraud matching is performed. All transactions are labeled as Fraud=0."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:38:17.578701Z",
     "start_time": "2025-11-20T05:38:17.004047Z"
    }
   },
   "source": [
    "# Reorganize by member with Fraud=0\n",
    "num_members = fr_inf.main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 3-1: FRAUD RELABELING & REORGANIZE BY MEMBER\n",
      "============================================================\n",
      "Input directory:  /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/clustered_out\n",
      "Output directory: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/by_member\n",
      "Found 1 CSV file(s)\n",
      "\n",
      "[1/1] Processing 01-01-2025_to_01-01-2025.csv\n",
      "\n",
      "Modified 300 member file(s) this run\n",
      "Sorting modified member files by Post Date / Post Time...\n",
      "  Sorted 300/300 member file(s)\n",
      "\n",
      "============================================================\n",
      "STAGE 3-1 COMPLETE\n",
      "============================================================\n",
      "Member files modified & sorted: 300\n",
      "Output location: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/by_member\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Encoding\n",
    "\n",
    "This stage encodes categorical features and prepares the final dataset for model prediction:\n",
    "\n",
    "1. **Remove ID Columns**: Delete Account ID and Member ID\n",
    "2. **Encode Categorical Features**: Convert categorical columns to numeric using predefined dictionary\n",
    "   - Account Type, Action Type, Source Type, Product ID\n",
    "3. **Parse Time Features**: Convert Post Time to decimal hours\n",
    "4. **Convert Date Features**: Parse Post Date and Account Open Date\n",
    "5. **Clean Up**: Remove text columns (Transaction Description, Fraud Adjustment Indicator)\n",
    "\n",
    "**Data Flow**: `data/pred/by_member/` → `data/pred/final/`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T05:38:21.447144Z",
     "start_time": "2025-11-20T05:38:20.510173Z"
    }
   },
   "source": [
    "# Encode features for prediction\n",
    "total_processed = enc_inf.encode_features(\n",
    "    enc_inf.PROCESSED_DIR,\n",
    "    enc_inf.OUTPUT_DIR,\n",
    "    enc_inf.CONFIG_PATH\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 4: FEATURE ENCODING (INFERENCE)\n",
      "============================================================\n",
      "Input Dir: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/by_member\n",
      "Output Dir: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/final\n",
      "Config Path: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/config/tokenize_dict.json\n",
      "\n",
      "Loaded encoding dictionary with 4 features\n",
      "\n",
      "Found 300 member file(s)\n",
      "  Processed 100/300 files\n",
      "  Processed 200/300 files\n",
      "  Processed 300/300 files\n",
      "\n",
      "============================================================\n",
      "STAGE 4 COMPLETE (INFERENCE)\n",
      "============================================================\n",
      "Total files processed: 300/300\n",
      "Output location: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/pred/final\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline Complete!\n",
    "\n",
    "The complete prediction preprocessing pipeline consists of 4 stages:\n",
    "\n",
    "1. **Data Cleaning**: Raw CSV → Cleaned CSV (`data/pred/cleaned/`)\n",
    "2. **Feature Engineering (Inference)**: Cleaned CSV → Clustered CSV (`data/pred/clustered_out/`)\n",
    "3. **Reorganize by Member**: Clustered CSV → Organized by Member (`data/pred/by_member/`)\n",
    "4. **Feature Encoding**: Processed CSV → Final Encoded Dataset (`data/pred/final/`)\n",
    "\n",
    "**Final Output**: `data/pred/final/member_*.csv`\n",
    "\n",
    "These final encoded files are ready for model prediction!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
