{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for Data Preprocessing",
   "id": "d7225e284319416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Import and Configuration",
   "id": "1c512e97635def66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T22:25:39.855023Z",
     "start_time": "2025-11-18T22:25:32.089669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "\n",
    "# Import data cleaning module\n",
    "sys.path.insert(0, os.path.abspath('./01_data_cleaning'))\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"data_cleaning\",\n",
    "    \"./01_data_cleaning/01_data_cleaning.py\"\n",
    ")\n",
    "dc = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(dc)\n",
    "\n",
    "# Import feature engineering module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"feature_engineering\",\n",
    "    \"02_feature_engineering/02b_description_encoding/test_run_pipeline.py\"\n",
    ")\n",
    "fe = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(fe)\n",
    "\n",
    "# Import fraud relabeling module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"fraud_relabeling\",\n",
    "    \"03_fraud_relabeling/03_fraud_relabeling.py\"\n",
    ")\n",
    "fr = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(fr)\n",
    "\n",
    "# Import encoding module\n",
    "spec = importlib.util.spec_from_file_location(\n",
    "    \"encoding\",\n",
    "    \"04_encoding/04_encoding.py\"\n",
    ")\n",
    "enc = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(enc)\n",
    "\n",
    "# Configure data cleaning\n",
    "dc.ENABLE_RENAMING = True\n",
    "dc.RAW_DIR = '../../data/pred/raw'\n",
    "dc.CLEANED_DIR = '../../data/pred/cleaned'\n",
    "\n",
    "# Configure feature engineering\n",
    "fe.PROCESSED_DIR = '../../data/pred/cleaned'  # Read from cleaned data\n",
    "fe.MODEL_NAME = 'prajjwal1/bert-tiny'\n",
    "fe.TEXT_COLUMN = 'Transaction Description'\n",
    "fe.BATCH_SIZE = 64\n",
    "fe.MAX_LENGTH = 64\n",
    "fe.PCA_DIM = 20\n",
    "fe.MIN_K = 10\n",
    "fe.MAX_K = 60\n",
    "fe.K_STEP = 10\n",
    "fe.SAMPLE_SIZE = 10000\n",
    "fe.CLUSTER_BATCH_SIZE = 4096\n",
    "fe.RANDOM_STATE = 42\n",
    "\n",
    "# Configure fraud relabeling\n",
    "fr.INPUT_DIR = '../../data/pred/clustered_out'  # Read from feature engineering output\n",
    "fr.OUTPUT_MEMBER_DIR = '../../data/pred/by_member'\n",
    "fr.OUTPUT_PROCESSED_DIR = '../../data/pred/processed'\n",
    "fr.CHUNKSIZE = 50000\n",
    "\n",
    "# Configure encoding\n",
    "enc.PROCESSED_DIR = '../../data/pred/processed'  # Read from fraud relabeling output\n",
    "enc.OUTPUT_DIR = '../../data/pred/final'\n",
    "enc.CONFIG_PATH = '../../config/tokenize_dict.json'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "directories = [\n",
    "    dc.RAW_DIR,\n",
    "    dc.CLEANED_DIR,\n",
    "    fr.INPUT_DIR,\n",
    "    fr.OUTPUT_MEMBER_DIR,\n",
    "    fr.OUTPUT_PROCESSED_DIR,\n",
    "    enc.OUTPUT_DIR\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")"
   ],
   "id": "179ab9ab779e8ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../../data/pred/raw\n",
      "Created directory: ../../data/pred/cleaned\n",
      "Created directory: ../../data/pred/clustered_out\n",
      "Created directory: ../../data/pred/by_member\n",
      "Created directory: ../../data/pred/processed\n",
      "Created directory: ../../data/pred/final\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data Cleaning\n\nThis cell performs the following preprocessing tasks:\n1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n2. Fix comma issues (remove extra commas in field values)\n3. Clean Amount field (remove $ and commas, convert to numeric)\n4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n5. Rename files based on date range (MM-DD-YYYY-MM-DD-YYYY.csv)\n\n**Input**: Raw CSV files from `../../data/raw/`  \n**Output**: Cleaned and renamed CSV files saved to `../../data/cleaned/`",
   "id": "50b577d75aae631e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:48:48.270501Z",
     "start_time": "2025-11-12T04:48:19.285895Z"
    }
   },
   "cell_type": "code",
   "source": "dc.main()",
   "id": "8ec45af1c8067334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ../../data/raw\n",
      "Cleaned: ../../data/cleaned\n",
      "\n",
      "Found 1 CSV files in ../../data/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. Transaction Data 09232025 (5).csv (63.05 MB)\n",
      "\n",
      "============================================================\n",
      "Processing Files...\n",
      "============================================================\n",
      "\n",
      "[1/1] Transaction Data 09232025 (5).csv... Missing:28077, →09-01-2024_to_09-01-2025.csv\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaned File Analysis",
   "id": "c372e04ff766b0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Size & Rows & Date Span",
   "id": "3b312b770f289f51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Overlapping detection",
   "id": "77b6792970472928"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Feature Engineering: Description Encoding and Clustering\n\nThis stage performs advanced feature engineering on transaction descriptions:\n\n1. **BERT Encoding**: Use BERT-tiny model to encode \"Transaction Description\" text into embeddings\n2. **Dimensionality Reduction**: Apply PCA to reduce embedding dimensions (default: 20D)\n3. **Automatic Clustering**: Find optimal cluster count (k) via heuristic search and cluster with MiniBatchKMeans\n4. **Add Cluster ID**: Append `cluster_id` column to each CSV file\n\n**Input**: Cleaned CSV files from `../../data/cleaned/`  \n**Output**: Clustered files saved to `../../data/clustered_out/`\n\n**Note**: This step requires GPU/CPU compute and may take significant time depending on data size.",
   "id": "e268885f085a0a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:49:50.883544Z",
     "start_time": "2025-11-12T04:49:05.988133Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = fe.main()",
   "id": "58b1d5fb190b6804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: DESCRIPTION ENCODING AND CLUSTERING\n",
      "============================================================\n",
      "Input: ../../data/cleaned\n",
      "Model: prajjwal1/bert-tiny\n",
      "Text Column: Transaction Description\n",
      "PCA Dimensions: 20\n",
      "Cluster Range: 10-60 (step 10)\n",
      "\n",
      "[Scan] Found 1 CSV file(s) in ../../data/cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Saved 1 clustered file(s) to /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/clustered_out\n",
      "\n",
      "============================================================\n",
      "STAGE 2 COMPLETE\n",
      "============================================================\n",
      "Processed 1 files\n",
      "Output location: ../../data/cleaned\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Fraud Matching and Re-label\nThis cell performs fraud detection in two stages:\n\n1. **Reorganize by Member**: Group all transactions by Member ID into individual files\n2. **Match Fraud Adjustments**: Find and mark original fraudulent transactions for each refund record （>10）\n   - Match by amount and date (extract from description or 30-day range)\n   - Prevent duplicate matching\n   - Categorize as matched/unmatched/no_fraud\n\n**Input**: Clustered files from `../../data/clustered_out/`  \n**Output**: Processed member files saved to `../../data/processed/[matched|unmatched|no_fraud]/`",
   "id": "64b755731928b893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage 3-1: Reorganize transactions by member\n",
    "\n",
    "**Input**: Clustered files from `../../data/clustered_out/`  \n",
    "**Output**: Member-grouped files saved to `../../data/by_member/`"
   ],
   "id": "f60bb8ed29d09f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:02:02.486282Z",
     "start_time": "2025-11-12T00:51:08.335915Z"
    }
   },
   "cell_type": "code",
   "source": "num_members = fr.run_stage1()",
   "id": "7156ce60494636e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1: DATA REORGANIZATION\n",
      "============================================================\n",
      "Input: ../../data/clustered_out\n",
      "Output: ../../data/by_member\n",
      "\n",
      "Found 4 files\n",
      "Processing 1/4: 01-01-2025_to_03-31-2025.csv\n",
      "Processing 2/4: 04-01-2025_to_06-30-2025.csv\n",
      "Processing 3/4: 07-01-2025_to_09-01-2025.csv\n",
      "Processing 4/4: 09-01-2024_to_12-31-2024.csv\n",
      "Created 29656 member files\n",
      "Sorting files...\n",
      "  Sorted 1000/29656 files\n",
      "  Sorted 2000/29656 files\n",
      "  Sorted 3000/29656 files\n",
      "  Sorted 4000/29656 files\n",
      "  Sorted 5000/29656 files\n",
      "  Sorted 6000/29656 files\n",
      "  Sorted 7000/29656 files\n",
      "  Sorted 8000/29656 files\n",
      "  Sorted 9000/29656 files\n",
      "  Sorted 10000/29656 files\n",
      "  Sorted 11000/29656 files\n",
      "  Sorted 12000/29656 files\n",
      "  Sorted 13000/29656 files\n",
      "  Sorted 14000/29656 files\n",
      "  Sorted 15000/29656 files\n",
      "  Sorted 16000/29656 files\n",
      "  Sorted 17000/29656 files\n",
      "  Sorted 18000/29656 files\n",
      "  Sorted 19000/29656 files\n",
      "  Sorted 20000/29656 files\n",
      "  Sorted 21000/29656 files\n",
      "  Sorted 22000/29656 files\n",
      "  Sorted 23000/29656 files\n",
      "  Sorted 24000/29656 files\n",
      "  Sorted 25000/29656 files\n",
      "  Sorted 26000/29656 files\n",
      "  Sorted 27000/29656 files\n",
      "  Sorted 28000/29656 files\n",
      "  Sorted 29000/29656 files\n",
      "\n",
      "29656 member files created\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "ou5w7jmthd8",
   "source": "## 4. Feature Encoding\n\nThis stage encodes categorical features and prepares the final dataset for model training:\n\n1. **Remove ID Columns**: Delete Account ID and Member ID\n2. **Encode Categorical Features**: Convert categorical columns to numeric using predefined dictionary\n   - Account Type, Action Type, Source Type, Product ID\n3. **Parse Time Features**: Convert Post Time to decimal hours\n4. **Convert Date Features**: Parse Post Date and Account Open Date\n5. **Clean Up**: Remove text columns (Transaction Description, Fraud Adjustment Indicator)\n\n**Input**: Processed member files from `../../data/processed/[matched|unmatched|no_fraud]/`  \n**Output**: Final encoded files saved to `../../data/final/[matched|unmatched|no_fraud]/`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cmq993u1rh",
   "source": "total_processed = enc.encode_features(enc.PROCESSED_DIR, enc.OUTPUT_DIR, enc.CONFIG_PATH)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:35:54.780503Z",
     "start_time": "2025-11-12T01:28:24.508706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENCODING\n",
      "============================================================\n",
      "Input Dir: ../../data/processed\n",
      "Output Dir: ../../data/final\n",
      "Config Path: ../../config/tokenize_dict.json\n",
      "\n",
      "Loaded encoding dictionary with 4 features\n",
      "\n",
      "matched: Found 544 files\n",
      "  Processed 100/544 files\n",
      "  Processed 200/544 files\n",
      "  Processed 300/544 files\n",
      "  Processed 400/544 files\n",
      "  Processed 500/544 files\n",
      "  matched: Encoded 544/544 files\n",
      "\n",
      "unmatched: Found 78 files\n",
      "  unmatched: Encoded 78/78 files\n",
      "\n",
      "no_fraud: Found 22673 files\n",
      "  Processed 100/22673 files\n",
      "  Processed 200/22673 files\n",
      "  Processed 300/22673 files\n",
      "  Processed 400/22673 files\n",
      "  Processed 500/22673 files\n",
      "  Processed 600/22673 files\n",
      "  Processed 700/22673 files\n",
      "  Processed 800/22673 files\n",
      "  Processed 900/22673 files\n",
      "  Processed 1000/22673 files\n",
      "  Processed 1100/22673 files\n",
      "  Processed 1200/22673 files\n",
      "  Processed 1300/22673 files\n",
      "  Processed 1400/22673 files\n",
      "  Processed 1500/22673 files\n",
      "  Processed 1600/22673 files\n",
      "  Processed 1700/22673 files\n",
      "  Processed 1800/22673 files\n",
      "  Processed 1900/22673 files\n",
      "  Processed 2000/22673 files\n",
      "  Processed 2100/22673 files\n",
      "  Processed 2200/22673 files\n",
      "  Processed 2300/22673 files\n",
      "  Processed 2400/22673 files\n",
      "  Processed 2500/22673 files\n",
      "  Processed 2600/22673 files\n",
      "  Processed 2700/22673 files\n",
      "  Processed 2800/22673 files\n",
      "  Processed 2900/22673 files\n",
      "  Processed 3000/22673 files\n",
      "  Processed 3100/22673 files\n",
      "  Processed 3200/22673 files\n",
      "  Processed 3300/22673 files\n",
      "  Processed 3400/22673 files\n",
      "  Processed 3500/22673 files\n",
      "  Processed 3600/22673 files\n",
      "  Processed 3700/22673 files\n",
      "  Processed 3800/22673 files\n",
      "  Processed 3900/22673 files\n",
      "  Processed 4000/22673 files\n",
      "  Processed 4100/22673 files\n",
      "  Processed 4200/22673 files\n",
      "  Processed 4300/22673 files\n",
      "  Processed 4400/22673 files\n",
      "  Processed 4500/22673 files\n",
      "  Processed 4600/22673 files\n",
      "  Processed 4700/22673 files\n",
      "  Processed 4800/22673 files\n",
      "  Processed 4900/22673 files\n",
      "  Processed 5000/22673 files\n",
      "  Processed 5100/22673 files\n",
      "  Processed 5200/22673 files\n",
      "  Processed 5300/22673 files\n",
      "  Processed 5400/22673 files\n",
      "  Processed 5500/22673 files\n",
      "  Processed 5600/22673 files\n",
      "  Processed 5700/22673 files\n",
      "  Processed 5800/22673 files\n",
      "  Processed 5900/22673 files\n",
      "  Processed 6000/22673 files\n",
      "  Processed 6100/22673 files\n",
      "  Processed 6200/22673 files\n",
      "  Processed 6300/22673 files\n",
      "  Processed 6400/22673 files\n",
      "  Processed 6500/22673 files\n",
      "  Processed 6600/22673 files\n",
      "  Processed 6700/22673 files\n",
      "  Processed 6800/22673 files\n",
      "  Processed 6900/22673 files\n",
      "  Processed 7000/22673 files\n",
      "  Processed 7100/22673 files\n",
      "  Processed 7200/22673 files\n",
      "  Processed 7300/22673 files\n",
      "  Processed 7400/22673 files\n",
      "  Processed 7500/22673 files\n",
      "  Processed 7600/22673 files\n",
      "  Processed 7700/22673 files\n",
      "  Processed 7800/22673 files\n",
      "  Processed 7900/22673 files\n",
      "  Processed 8000/22673 files\n",
      "  Processed 8100/22673 files\n",
      "  Processed 8200/22673 files\n",
      "  Processed 8300/22673 files\n",
      "  Processed 8400/22673 files\n",
      "  Processed 8500/22673 files\n",
      "  Processed 8600/22673 files\n",
      "  Processed 8700/22673 files\n",
      "  Processed 8800/22673 files\n",
      "  Processed 8900/22673 files\n",
      "  Processed 9000/22673 files\n",
      "  Processed 9100/22673 files\n",
      "  Processed 9200/22673 files\n",
      "  Processed 9300/22673 files\n",
      "  Processed 9400/22673 files\n",
      "  Processed 9500/22673 files\n",
      "  Processed 9600/22673 files\n",
      "  Processed 9700/22673 files\n",
      "  Processed 9800/22673 files\n",
      "  Processed 9900/22673 files\n",
      "  Processed 10000/22673 files\n",
      "  Processed 10100/22673 files\n",
      "  Processed 10200/22673 files\n",
      "  Processed 10300/22673 files\n",
      "  Processed 10400/22673 files\n",
      "  Processed 10500/22673 files\n",
      "  Processed 10600/22673 files\n",
      "  Processed 10700/22673 files\n",
      "  Processed 10800/22673 files\n",
      "  Processed 10900/22673 files\n",
      "  Processed 11000/22673 files\n",
      "  Processed 11100/22673 files\n",
      "  Processed 11200/22673 files\n",
      "  Processed 11300/22673 files\n",
      "  Processed 11400/22673 files\n",
      "  Processed 11500/22673 files\n",
      "  Processed 11600/22673 files\n",
      "  Processed 11700/22673 files\n",
      "  Processed 11800/22673 files\n",
      "  Processed 11900/22673 files\n",
      "  Processed 12000/22673 files\n",
      "  Processed 12100/22673 files\n",
      "  Processed 12200/22673 files\n",
      "  Processed 12300/22673 files\n",
      "  Processed 12400/22673 files\n",
      "  Processed 12500/22673 files\n",
      "  Processed 12600/22673 files\n",
      "  Processed 12700/22673 files\n",
      "  Processed 12800/22673 files\n",
      "  Processed 12900/22673 files\n",
      "  Processed 13000/22673 files\n",
      "  Processed 13100/22673 files\n",
      "  Processed 13200/22673 files\n",
      "  Processed 13300/22673 files\n",
      "  Processed 13400/22673 files\n",
      "  Processed 13500/22673 files\n",
      "  Processed 13600/22673 files\n",
      "  Processed 13700/22673 files\n",
      "  Processed 13800/22673 files\n",
      "  Processed 13900/22673 files\n",
      "  Processed 14000/22673 files\n",
      "  Processed 14100/22673 files\n",
      "  Processed 14200/22673 files\n",
      "  Processed 14300/22673 files\n",
      "  Processed 14400/22673 files\n",
      "  Processed 14500/22673 files\n",
      "  Processed 14600/22673 files\n",
      "  Processed 14700/22673 files\n",
      "  Processed 14800/22673 files\n",
      "  Processed 14900/22673 files\n",
      "  Processed 15000/22673 files\n",
      "  Processed 15100/22673 files\n",
      "  Processed 15200/22673 files\n",
      "  Processed 15300/22673 files\n",
      "  Processed 15400/22673 files\n",
      "  Processed 15500/22673 files\n",
      "  Processed 15600/22673 files\n",
      "  Processed 15700/22673 files\n",
      "  Processed 15800/22673 files\n",
      "  Processed 15900/22673 files\n",
      "  Processed 16000/22673 files\n",
      "  Processed 16100/22673 files\n",
      "  Processed 16200/22673 files\n",
      "  Processed 16300/22673 files\n",
      "  Processed 16400/22673 files\n",
      "  Processed 16500/22673 files\n",
      "  Processed 16600/22673 files\n",
      "  Processed 16700/22673 files\n",
      "  Processed 16800/22673 files\n",
      "  Processed 16900/22673 files\n",
      "  Processed 17000/22673 files\n",
      "  Processed 17100/22673 files\n",
      "  Processed 17200/22673 files\n",
      "  Processed 17300/22673 files\n",
      "  Processed 17400/22673 files\n",
      "  Processed 17500/22673 files\n",
      "  Processed 17600/22673 files\n",
      "  Processed 17700/22673 files\n",
      "  Processed 17800/22673 files\n",
      "  Processed 17900/22673 files\n",
      "  Processed 18000/22673 files\n",
      "  Processed 18100/22673 files\n",
      "  Processed 18200/22673 files\n",
      "  Processed 18300/22673 files\n",
      "  Processed 18400/22673 files\n",
      "  Processed 18500/22673 files\n",
      "  Processed 18600/22673 files\n",
      "  Processed 18700/22673 files\n",
      "  Processed 18800/22673 files\n",
      "  Processed 18900/22673 files\n",
      "  Processed 19000/22673 files\n",
      "  Processed 19100/22673 files\n",
      "  Processed 19200/22673 files\n",
      "  Processed 19300/22673 files\n",
      "  Processed 19400/22673 files\n",
      "  Processed 19500/22673 files\n",
      "  Processed 19600/22673 files\n",
      "  Processed 19700/22673 files\n",
      "  Processed 19800/22673 files\n",
      "  Processed 19900/22673 files\n",
      "  Processed 20000/22673 files\n",
      "  Processed 20100/22673 files\n",
      "  Processed 20200/22673 files\n",
      "  Processed 20300/22673 files\n",
      "  Processed 20400/22673 files\n",
      "  Processed 20500/22673 files\n",
      "  Processed 20600/22673 files\n",
      "  Processed 20700/22673 files\n",
      "  Processed 20800/22673 files\n",
      "  Processed 20900/22673 files\n",
      "  Processed 21000/22673 files\n",
      "  Processed 21100/22673 files\n",
      "  Processed 21200/22673 files\n",
      "  Processed 21300/22673 files\n",
      "  Processed 21400/22673 files\n",
      "  Processed 21500/22673 files\n",
      "  Processed 21600/22673 files\n",
      "  Processed 21700/22673 files\n",
      "  Processed 21800/22673 files\n",
      "  Processed 21900/22673 files\n",
      "  Processed 22000/22673 files\n",
      "  Processed 22100/22673 files\n",
      "  Processed 22200/22673 files\n",
      "  Processed 22300/22673 files\n",
      "  Processed 22400/22673 files\n",
      "  Processed 22500/22673 files\n",
      "  Processed 22600/22673 files\n",
      "  no_fraud: Encoded 22673/22673 files\n",
      "\n",
      "============================================================\n",
      "Encoding Complete!\n",
      "Total files found: 23295\n",
      "Total files processed: 23295\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "75bd63344823637a",
   "source": "TODO : vulnerability scanner",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Pipeline Complete!\n",
    "\n",
    "The complete data preprocessing pipeline consists of 4 stages:\n",
    "\n",
    "1. **Data Cleaning**: Raw CSV → Cleaned CSV (`data/cleaned/`)\n",
    "2. **Feature Engineering**: Cleaned CSV → Clustered CSV (`data/clustered_out/`)\n",
    "3. **Fraud Matching**: Clustered CSV → Categorized by Member (`data/processed/`)\n",
    "4. **Feature Encoding**: Processed CSV → Final Encoded Dataset (`data/final/`)\n",
    "\n",
    "**Final Output**: `../../data/final/[matched|unmatched|no_fraud]/member_*.csv`\n",
    "\n",
    "These final encoded files are ready for model training!"
   ],
   "id": "5o3ixzv5ahs"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
