{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for Data Preprocessing",
   "id": "d7225e284319416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Required Libraries",
   "id": "1c512e97635def66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T07:31:17.929263Z",
     "start_time": "2025-10-20T07:31:17.924732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "raw_dir = '../../data/raw'\n",
    "cleaned_dir = '../../data/cleaned'\n",
    "\n",
    "# Create cleaned directory if not exists\n",
    "os.makedirs(cleaned_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Raw: {raw_dir}\")\n",
    "print(f\"Cleaned: {cleaned_dir}\")"
   ],
   "id": "179ab9ab779e8ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ../../data/raw\n",
      "Cleaned: ../../data/cleaned\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scan Directory and List All CSV Files",
   "id": "db17725c13c2e129"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T07:31:19.746107Z",
     "start_time": "2025-10-20T07:31:19.741005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if directory exists\n",
    "if not os.path.exists(raw_dir):\n",
    "    print(f\"Error: Directory {raw_dir} does not exist\")\n",
    "    csv_files = []\n",
    "else:\n",
    "    # Get all CSV files\n",
    "    csv_files = [f for f in os.listdir(raw_dir) if f.endswith('.csv')]\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files in {raw_dir}\")\n",
    "    print(\"\\nCSV Files List ⬇️\")\n",
    "    for i, filename in enumerate(csv_files, 1):\n",
    "        file_size = os.path.getsize(os.path.join(raw_dir, filename)) / (1024 * 1024)  # MB\n",
    "        print(f\"  {i}. {filename} ({file_size:.2f} MB)\")"
   ],
   "id": "70a615ce7ff1e62b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 CSV files in ../../data/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. TransactionData 10-9-25.csv (601.85 MB)\n",
      "  2. TransactionData10-3-25.csv (442.34 MB)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess CSV Files\n",
    "This cell performs the following preprocessing tasks:\n",
    "1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n",
    "2. Fix comma issues (remove extra commas in field values)\n",
    "3. Clean Amount field (remove $ and commas, convert to numeric)\n",
    "4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n",
    "5. Rename files based on date range (MM-DD-YYYY-MM-DD-YYYY.csv)\n",
    "\n",
    "Output: Cleaned and renamed CSV files saved to ../../data/cleaned/\n"
   ],
   "id": "50b577d75aae631e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T09:27:41.942892Z",
     "start_time": "2025-10-20T07:31:24.167962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ENABLE_RENAMING = True\n",
    "\n",
    "STANDARD_HEADERS = [\n",
    "    'Account ID', 'Member ID', 'Account Type', 'Account Open Date',\n",
    "    'Member Age', 'Product ID', 'Post Date', 'Post Time', 'Amount',\n",
    "    'Action Type', 'Source Type', 'Transaction Description',\n",
    "    'Fraud Adjustment Indicator'\n",
    "]\n",
    "\n",
    "for i, filename in enumerate(csv_files, 1):\n",
    "    file_path = os.path.join(raw_dir, filename)\n",
    "    print(f\"[{i}/{len(csv_files)}] {filename}...\", end=' ')\n",
    "\n",
    "    # Step 1: Pre-process raw lines to clean Amount field\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    processed_lines = []\n",
    "    fixed_amount = 0\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        if idx == 0:  # Keep header as-is\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        # Find and clean amount patterns like \"$1,000.00\" or \"$500\"\n",
    "        # Pattern: optional quote + $ + digits with optional commas + optional .decimals + optional quote\n",
    "        def clean_amount(match):\n",
    "            amount_text = match.group(0)\n",
    "            # Remove quotes, $, commas\n",
    "            cleaned = amount_text.replace('\"', '').replace('$', '').replace(',', '').strip()\n",
    "            return cleaned\n",
    "\n",
    "        # Match quoted or unquoted amounts\n",
    "        new_line = re.sub(r'\"\\$[\\d,]+\\.?\\d*\\s*\"|\\$[\\d,]+\\.?\\d*', clean_amount, line)\n",
    "\n",
    "        if new_line != line:\n",
    "            fixed_amount += 1\n",
    "\n",
    "        processed_lines.append(new_line)\n",
    "\n",
    "    # Step 2: Parse with CSV module\n",
    "    from io import StringIO\n",
    "    csv_text = ''.join(processed_lines)\n",
    "    reader = csv.reader(StringIO(csv_text))\n",
    "    all_rows = list(reader)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"EMPTY\")\n",
    "        continue\n",
    "\n",
    "    # Standardize headers\n",
    "    header_fields = all_rows[0]\n",
    "    normalized_header = []\n",
    "    for field in header_fields:\n",
    "        field_clean = field.strip().replace(' ', '')\n",
    "        matched = False\n",
    "        for std in STANDARD_HEADERS:\n",
    "            if field_clean.lower() == std.replace(' ', '').lower():\n",
    "                normalized_header.append(std)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            normalized_header.append(field.strip())\n",
    "\n",
    "    # Get column indices\n",
    "    try:\n",
    "        idx_map = {h: normalized_header.index(h) for h in ['Amount', 'Product ID', 'Action Type',\n",
    "                   'Transaction Description', 'Account Type', 'Post Date']}\n",
    "    except ValueError:\n",
    "        idx_map = {}\n",
    "\n",
    "    cleaned_rows = [normalized_header]\n",
    "    fixed_comma = fixed_missing = 0\n",
    "    first_date = last_date = None\n",
    "\n",
    "    # Process data rows\n",
    "    for row in all_rows[1:]:\n",
    "        fields = list(row)\n",
    "\n",
    "        # Remove empty trailing field (from trailing comma)\n",
    "        while len(fields) > 13 and not fields[-1].strip():\n",
    "            fields.pop()\n",
    "\n",
    "        # Merge extra fields from end\n",
    "        while len(fields) > 13:\n",
    "            fields[-3] = fields[-3] + ' ' + fields[-2]\n",
    "            fields.pop(-2)\n",
    "            fixed_comma += 1\n",
    "\n",
    "        # Pad if needed\n",
    "        while len(fields) < 13:\n",
    "            fields.append('')\n",
    "\n",
    "        # Extract dates\n",
    "        if 'Post Date' in idx_map:\n",
    "            try:\n",
    "                date_val = pd.to_datetime(fields[idx_map['Post Date']])\n",
    "                if first_date is None:\n",
    "                    first_date = date_val\n",
    "                last_date = date_val\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Fill missing values\n",
    "        fill_rules = [\n",
    "            ('Amount', '0'),\n",
    "            ('Product ID', 'Unknown'),\n",
    "            ('Action Type', 'Unknown'),\n",
    "            ('Transaction Description', 'Unknown'),\n",
    "            ('Account Type', 'Unknown')\n",
    "        ]\n",
    "\n",
    "        for col, fill_val in fill_rules:\n",
    "            if col in idx_map:\n",
    "                idx = idx_map[col]\n",
    "                if 0 <= idx < len(fields):\n",
    "                    val = fields[idx].strip()\n",
    "                    if not val or val.lower() == 'null':\n",
    "                        fields[idx] = fill_val\n",
    "                        fixed_missing += 1\n",
    "\n",
    "        cleaned_rows.append(fields)\n",
    "\n",
    "    # Determine output filename\n",
    "    output_filename = filename\n",
    "    if ENABLE_RENAMING and first_date and last_date:\n",
    "        output_filename = f\"{first_date.strftime('%m-%d-%Y')}-{last_date.strftime('%m-%d-%Y')}.csv\"\n",
    "\n",
    "    # Save\n",
    "    output_path = os.path.join(cleaned_dir, output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(cleaned_rows)\n",
    "\n",
    "    # Output summary\n",
    "    msg = []\n",
    "    if fixed_comma > 0:\n",
    "        msg.append(f\"Fields:{fixed_comma}\")\n",
    "    if fixed_amount > 0:\n",
    "        msg.append(f\"Amount:{fixed_amount}\")\n",
    "    if fixed_missing > 0:\n",
    "        msg.append(f\"Missing:{fixed_missing}\")\n",
    "    if ENABLE_RENAMING and output_filename != filename:\n",
    "        msg.append(f\"→{output_filename}\")\n",
    "    print(', '.join(msg) if msg else \"OK\")"
   ],
   "id": "8ec45af1c8067334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] TransactionData 10-9-25.csv... Fields:221, Amount:95, Missing:113579, →01-01-2025-12-31-2024.csv\n",
      "[2/2] TransactionData10-3-25.csv... Amount:238, Missing:71518, →08-01-2025-09-01-2025.csv\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyze Each CSV File",
   "id": "9864ab76053655b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = []\n",
    "\n",
    "for i, filename in enumerate(csv_files, 1):\n",
    "    cleaned_path = os.path.join(cleaned_dir, filename)\n",
    "    print(f\"\\n[{i}/{len(csv_files)}] {filename}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(cleaned_path, low_memory=False)\n",
    "\n",
    "        # Get first and last date (data is sorted by Post Date)\n",
    "        first_date = pd.to_datetime(df['Post Date'].iloc[0])\n",
    "        last_date = pd.to_datetime(df['Post Date'].iloc[-1])\n",
    "\n",
    "        total = len(df)\n",
    "        fraud = df['Fraud Adjustment Indicator'].notna().sum()\n",
    "        pct = (fraud / total * 100) if total > 0 else 0\n",
    "\n",
    "        new_name = f\"{first_date.strftime('%m-%d-%Y')}-{last_date.strftime('%m-%d-%Y')}.csv\"\n",
    "\n",
    "        results.append({\n",
    "            'original': filename,\n",
    "            'new_name': new_name,\n",
    "            'first_date': first_date,\n",
    "            'last_date': last_date,\n",
    "            'total': total,\n",
    "            'fraud': fraud,\n",
    "            'pct': round(pct, 4)\n",
    "        })\n",
    "\n",
    "        print(f\"  Date: {first_date.strftime('%m/%d/%Y')} - {last_date.strftime('%m/%d/%Y')}\")\n",
    "        print(f\"  Total: {total:,} | Fraud: {fraud:,} ({pct:.4f}%)\")\n",
    "        print(f\"  → {new_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")"
   ],
   "id": "adea2cb2d6eb352f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rename Files",
   "id": "33edd860dd1fa26a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ENABLE_RENAMING = False  # Set to True to rename\n",
    "\n",
    "if ENABLE_RENAMING:\n",
    "    for result in results:\n",
    "        old_path = os.path.join(raw_dir, result['original'])\n",
    "        new_path = os.path.join(raw_dir, result['new_name'])\n",
    "\n",
    "        if old_path != new_path and not os.path.exists(new_path):\n",
    "            shutil.move(old_path, new_path)\n",
    "            print(f\"✓ {result['original']} → {result['new_name']}\")\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Set ENABLE_RENAMING = True to rename\")"
   ],
   "id": "98c7febbbbcd67c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
