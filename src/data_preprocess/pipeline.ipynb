{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for Data Preprocessing",
   "id": "d7225e284319416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Import and Configuration",
   "id": "1c512e97635def66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:03:33.235721Z",
     "start_time": "2025-11-11T01:03:32.655045Z"
    }
   },
   "cell_type": "code",
   "source": "import sys\nimport os\nimport pandas as pd\nimport importlib.util\n\n# Import data cleaning module\nsys.path.insert(0, os.path.abspath('./01_data_cleaning'))\nspec = importlib.util.spec_from_file_location(\n    \"data_cleaning\",\n    \"./01_data_cleaning/01_data_cleaning.py\"\n)\ndc = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(dc)\n\n# Import fraud relabeling module\nspec = importlib.util.spec_from_file_location(\n    \"fraud_relabeling\",\n    \"./02_fraud_relabeling/02_fraud_relabeling.py\"\n)\nfr = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fr)\n\n# Import feature engineering module\nspec = importlib.util.spec_from_file_location(\n    \"feature_engineering\",\n    \"./03_feature_engineering/03_feature_engineering.py\"\n)\nfe = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fe)\n\n# Configure data cleaning\ndc.ENABLE_RENAMING = True\ndc.RAW_DIR = '../../data/raw'\ndc.CLEANED_DIR = '../../data/cleaned'\n\n# Configure fraud relabeling\nfr.INPUT_DIR = '../../data/cleaned'\nfr.OUTPUT_MEMBER_DIR = '../../data/by_member'\nfr.OUTPUT_PROCESSED_DIR = '../../data/processed'\nfr.CHUNKSIZE = 50000\n\n# Configure feature engineering\nfe.PROCESSED_DIR = '../../data/processed'\nfe.MODEL_NAME = 'prajjwal1/bert-tiny'\nfe.TEXT_COLUMN = 'Transaction Description'\nfe.BATCH_SIZE = 64\nfe.MAX_LENGTH = 64\nfe.PCA_DIM = 20\nfe.MIN_K = 10\nfe.MAX_K = 60\nfe.K_STEP = 10\nfe.SAMPLE_SIZE = 10000\nfe.CLUSTER_BATCH_SIZE = 4096\nfe.RANDOM_STATE = 42\n\n# Create directories if they don't exist\ndirectories = [\n    dc.RAW_DIR,\n    dc.CLEANED_DIR,\n    fr.INPUT_DIR,\n    fr.OUTPUT_MEMBER_DIR,\n    fr.OUTPUT_PROCESSED_DIR\n]\n\nfor directory in directories:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        print(f\"Created directory: {directory}\")",
   "id": "179ab9ab779e8ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../../data/cleaned\n",
      "Created directory: ../../data/by_member\n",
      "Created directory: ../../data/processed\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Cleaning\n",
    "This cell performs the following preprocessing tasks:\n",
    "1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n",
    "2. Fix comma issues (remove extra commas in field values)\n",
    "3. Clean Amount field (remove $ and commas, convert to numeric)\n",
    "4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n",
    "5. Rename files based on date range (MM-DD-YYYY-MM-DD-YYYY.csv)\n",
    "\n",
    "Output: Cleaned and renamed CSV files saved to ../../data/cleaned/\n"
   ],
   "id": "50b577d75aae631e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:05:43.931582Z",
     "start_time": "2025-11-11T01:03:50.839029Z"
    }
   },
   "cell_type": "code",
   "source": "dc.main()",
   "id": "8ec45af1c8067334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ../../data/raw\n",
      "Cleaned: ../../data/cleaned\n",
      "\n",
      "Found 1 CSV files in ../../data/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. TransactionData_2025Q1.csv (298.68 MB)\n",
      "\n",
      "============================================================\n",
      "Processing Files...\n",
      "============================================================\n",
      "\n",
      "[1/1] TransactionData_2025Q1.csv... Missing:100165, Newlines:2189448, →01-01-2025_to_03-31-2025.csv\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaned File Analysis",
   "id": "c372e04ff766b0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Size & Rows & Date Span",
   "id": "3b312b770f289f51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:06:46.221447Z",
     "start_time": "2025-11-11T01:06:44.369103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "# Load all dataframes\n",
    "dfs = {}\n",
    "for filename in csv_files:\n",
    "    dfs[filename] = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "\n",
    "# Collect stats\n",
    "stats = []\n",
    "for filename, df in dfs.items():\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    min_date = df['Post Date'].min()\n",
    "    max_date = df['Post Date'].max()\n",
    "    fraud_count = ((df['Fraud Adjustment Indicator'].notna()) &\n",
    "                   (df['Fraud Adjustment Indicator'] != '')).sum()\n",
    "\n",
    "    stats.append({\n",
    "        'File': filename,\n",
    "        'Rows': len(df),\n",
    "        'Members': df['Member ID'].nunique(),\n",
    "        'Date From': min_date.strftime('%m/%d/%Y') if pd.notna(min_date) else 'N/A',\n",
    "        'Date To': max_date.strftime('%m/%d/%Y') if pd.notna(max_date) else 'N/A',\n",
    "        'Days': (max_date - min_date).days if pd.notna(min_date) else 0,\n",
    "        'Fraud %': round(fraud_count / len(df) * 100, 4) if len(df) > 0 else 0\n",
    "    })\n",
    "\n",
    "# Display table\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "# Summary\n",
    "all_members = set()\n",
    "for df in dfs.values():\n",
    "    all_members.update(df['Member ID'].dropna())\n",
    "\n",
    "total_fraud = sum([((dfs[f]['Fraud Adjustment Indicator'].notna()) &\n",
    "                    (dfs[f]['Fraud Adjustment Indicator'] != '')).sum()\n",
    "                   for f in dfs.keys()])\n",
    "\n",
    "print(f\"\\nTotal Rows: {df_stats['Rows'].sum():,}\")\n",
    "print(f\"Total Unique Members: {len(all_members):,}\")\n",
    "print(f\"Total Fraud Indicators: {total_fraud:,}\")\n",
    "print(f\"Overall Fraud %: {round(total_fraud / df_stats['Rows'].sum() * 100, 4)}%\")"
   ],
   "id": "d9503066fa80edd1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           File     Rows  Members   Date From     Date To  \\\n",
       "0  01-01-2025_to_03-31-2025.csv  2189448    25572  01/01/2025  03/31/2025   \n",
       "\n",
       "   Days  Fraud %  \n",
       "0    89   0.0153  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Members</th>\n",
       "      <th>Date From</th>\n",
       "      <th>Date To</th>\n",
       "      <th>Days</th>\n",
       "      <th>Fraud %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01-01-2025_to_03-31-2025.csv</td>\n",
       "      <td>2189448</td>\n",
       "      <td>25572</td>\n",
       "      <td>01/01/2025</td>\n",
       "      <td>03/31/2025</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Rows: 2,189,448\n",
      "Total Unique Members: 25,572\n",
      "Total Fraud Indicators: 334\n",
      "Overall Fraud %: 0.0153%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Overlapping detection",
   "id": "77b6792970472928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:06:59.254373Z",
     "start_time": "2025-11-11T01:06:55.533958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "print(f\"Loading {len(csv_files)} files...\")\n",
    "\n",
    "# Load files and create row IDs\n",
    "file_rows = {}\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "    row_ids = set(df['Account ID'].astype(str) + '|' +\n",
    "                  df['Member ID'].astype(str) + '|' +\n",
    "                  df['Post Date'].astype(str) + '|' +\n",
    "                  df['Post Time'].astype(str) + '|' +\n",
    "                  df['Amount'].astype(str))\n",
    "    file_rows[filename] = row_ids\n",
    "\n",
    "# Calculate pairwise overlaps\n",
    "results = []\n",
    "for i, file1 in enumerate(csv_files):\n",
    "    for j, file2 in enumerate(csv_files):\n",
    "        if i < j:\n",
    "            overlap = len(file_rows[file1] & file_rows[file2])\n",
    "            pct1 = overlap / len(file_rows[file1]) * 100\n",
    "            pct2 = overlap / len(file_rows[file2]) * 100\n",
    "\n",
    "            results.append({\n",
    "                'File 1': file1,\n",
    "                'File 2': file2,\n",
    "                'Overlap Rows': overlap,\n",
    "                '% of File 1': round(pct1, 1),\n",
    "                '% of File 2': round(pct2, 1)\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display as styled DataFrame\n",
    "display(df_results)"
   ],
   "id": "34a3032caa9d448c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Fraud Matching and Re-lable\n",
    "This cell performs fraud detection in two stages:\n",
    "\n",
    "1. **Reorganize by Member**: Group all transactions by Member ID into individual files\n",
    "2. **Match Fraud Adjustments**: Find and mark original fraudulent transactions for each refund record （>10）\n",
    "   - Match by amount and date (extract from description or 30-day range)\n",
    "   - Prevent duplicate matching\n",
    "   - Categorize as matched/unmatched/no_fraud\n",
    "\n",
    "Output: Processed member files saved to `../../data/processed/[matched|unmatched|no_fraud]/`"
   ],
   "id": "64b755731928b893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 1: Reorganize transactions by member",
   "id": "f60bb8ed29d09f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:10:09.429425Z",
     "start_time": "2025-11-11T01:07:32.090976Z"
    }
   },
   "cell_type": "code",
   "source": "num_members = fr.run_stage1()",
   "id": "7156ce60494636e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1: DATA REORGANIZATION\n",
      "============================================================\n",
      "Input: ../../data/cleaned\n",
      "Output: ../../data/by_member\n",
      "\n",
      "Found 1 files\n",
      "Processing 1/1: 01-01-2025_to_03-31-2025.csv\n",
      "Created 25572 member files\n",
      "Sorting files...\n",
      "  Sorted 100/25572 files\n",
      "  Sorted 200/25572 files\n",
      "  Sorted 300/25572 files\n",
      "  Sorted 400/25572 files\n",
      "  Sorted 500/25572 files\n",
      "  Sorted 600/25572 files\n",
      "  Sorted 700/25572 files\n",
      "  Sorted 800/25572 files\n",
      "  Sorted 900/25572 files\n",
      "  Sorted 1000/25572 files\n",
      "  Sorted 1100/25572 files\n",
      "  Sorted 1200/25572 files\n",
      "  Sorted 1300/25572 files\n",
      "  Sorted 1400/25572 files\n",
      "  Sorted 1500/25572 files\n",
      "  Sorted 1600/25572 files\n",
      "  Sorted 1700/25572 files\n",
      "  Sorted 1800/25572 files\n",
      "  Sorted 1900/25572 files\n",
      "  Sorted 2000/25572 files\n",
      "  Sorted 2100/25572 files\n",
      "  Sorted 2200/25572 files\n",
      "  Sorted 2300/25572 files\n",
      "  Sorted 2400/25572 files\n",
      "  Sorted 2500/25572 files\n",
      "  Sorted 2600/25572 files\n",
      "  Sorted 2700/25572 files\n",
      "  Sorted 2800/25572 files\n",
      "  Sorted 2900/25572 files\n",
      "  Sorted 3000/25572 files\n",
      "  Sorted 3100/25572 files\n",
      "  Sorted 3200/25572 files\n",
      "  Sorted 3300/25572 files\n",
      "  Sorted 3400/25572 files\n",
      "  Sorted 3500/25572 files\n",
      "  Sorted 3600/25572 files\n",
      "  Sorted 3700/25572 files\n",
      "  Sorted 3800/25572 files\n",
      "  Sorted 3900/25572 files\n",
      "  Sorted 4000/25572 files\n",
      "  Sorted 4100/25572 files\n",
      "  Sorted 4200/25572 files\n",
      "  Sorted 4300/25572 files\n",
      "  Sorted 4400/25572 files\n",
      "  Sorted 4500/25572 files\n",
      "  Sorted 4600/25572 files\n",
      "  Sorted 4700/25572 files\n",
      "  Sorted 4800/25572 files\n",
      "  Sorted 4900/25572 files\n",
      "  Sorted 5000/25572 files\n",
      "  Sorted 5100/25572 files\n",
      "  Sorted 5200/25572 files\n",
      "  Sorted 5300/25572 files\n",
      "  Sorted 5400/25572 files\n",
      "  Sorted 5500/25572 files\n",
      "  Sorted 5600/25572 files\n",
      "  Sorted 5700/25572 files\n",
      "  Sorted 5800/25572 files\n",
      "  Sorted 5900/25572 files\n",
      "  Sorted 6000/25572 files\n",
      "  Sorted 6100/25572 files\n",
      "  Sorted 6200/25572 files\n",
      "  Sorted 6300/25572 files\n",
      "  Sorted 6400/25572 files\n",
      "  Sorted 6500/25572 files\n",
      "  Sorted 6600/25572 files\n",
      "  Sorted 6700/25572 files\n",
      "  Sorted 6800/25572 files\n",
      "  Sorted 6900/25572 files\n",
      "  Sorted 7000/25572 files\n",
      "  Sorted 7100/25572 files\n",
      "  Sorted 7200/25572 files\n",
      "  Sorted 7300/25572 files\n",
      "  Sorted 7400/25572 files\n",
      "  Sorted 7500/25572 files\n",
      "  Sorted 7600/25572 files\n",
      "  Sorted 7700/25572 files\n",
      "  Sorted 7800/25572 files\n",
      "  Sorted 7900/25572 files\n",
      "  Sorted 8000/25572 files\n",
      "  Sorted 8100/25572 files\n",
      "  Sorted 8200/25572 files\n",
      "  Sorted 8300/25572 files\n",
      "  Sorted 8400/25572 files\n",
      "  Sorted 8500/25572 files\n",
      "  Sorted 8600/25572 files\n",
      "  Sorted 8700/25572 files\n",
      "  Sorted 8800/25572 files\n",
      "  Sorted 8900/25572 files\n",
      "  Sorted 9000/25572 files\n",
      "  Sorted 9100/25572 files\n",
      "  Sorted 9200/25572 files\n",
      "  Sorted 9300/25572 files\n",
      "  Sorted 9400/25572 files\n",
      "  Sorted 9500/25572 files\n",
      "  Sorted 9600/25572 files\n",
      "  Sorted 9700/25572 files\n",
      "  Sorted 9800/25572 files\n",
      "  Sorted 9900/25572 files\n",
      "  Sorted 10000/25572 files\n",
      "  Sorted 10100/25572 files\n",
      "  Sorted 10200/25572 files\n",
      "  Sorted 10300/25572 files\n",
      "  Sorted 10400/25572 files\n",
      "  Sorted 10500/25572 files\n",
      "  Sorted 10600/25572 files\n",
      "  Sorted 10700/25572 files\n",
      "  Sorted 10800/25572 files\n",
      "  Sorted 10900/25572 files\n",
      "  Sorted 11000/25572 files\n",
      "  Sorted 11100/25572 files\n",
      "  Sorted 11200/25572 files\n",
      "  Sorted 11300/25572 files\n",
      "  Sorted 11400/25572 files\n",
      "  Sorted 11500/25572 files\n",
      "  Sorted 11600/25572 files\n",
      "  Sorted 11700/25572 files\n",
      "  Sorted 11800/25572 files\n",
      "  Sorted 11900/25572 files\n",
      "  Sorted 12000/25572 files\n",
      "  Sorted 12100/25572 files\n",
      "  Sorted 12200/25572 files\n",
      "  Sorted 12300/25572 files\n",
      "  Sorted 12400/25572 files\n",
      "  Sorted 12500/25572 files\n",
      "  Sorted 12600/25572 files\n",
      "  Sorted 12700/25572 files\n",
      "  Sorted 12800/25572 files\n",
      "  Sorted 12900/25572 files\n",
      "  Sorted 13000/25572 files\n",
      "  Sorted 13100/25572 files\n",
      "  Sorted 13200/25572 files\n",
      "  Sorted 13300/25572 files\n",
      "  Sorted 13400/25572 files\n",
      "  Sorted 13500/25572 files\n",
      "  Sorted 13600/25572 files\n",
      "  Sorted 13700/25572 files\n",
      "  Sorted 13800/25572 files\n",
      "  Sorted 13900/25572 files\n",
      "  Sorted 14000/25572 files\n",
      "  Sorted 14100/25572 files\n",
      "  Sorted 14200/25572 files\n",
      "  Sorted 14300/25572 files\n",
      "  Sorted 14400/25572 files\n",
      "  Sorted 14500/25572 files\n",
      "  Sorted 14600/25572 files\n",
      "  Sorted 14700/25572 files\n",
      "  Sorted 14800/25572 files\n",
      "  Sorted 14900/25572 files\n",
      "  Sorted 15000/25572 files\n",
      "  Sorted 15100/25572 files\n",
      "  Sorted 15200/25572 files\n",
      "  Sorted 15300/25572 files\n",
      "  Sorted 15400/25572 files\n",
      "  Sorted 15500/25572 files\n",
      "  Sorted 15600/25572 files\n",
      "  Sorted 15700/25572 files\n",
      "  Sorted 15800/25572 files\n",
      "  Sorted 15900/25572 files\n",
      "  Sorted 16000/25572 files\n",
      "  Sorted 16100/25572 files\n",
      "  Sorted 16200/25572 files\n",
      "  Sorted 16300/25572 files\n",
      "  Sorted 16400/25572 files\n",
      "  Sorted 16500/25572 files\n",
      "  Sorted 16600/25572 files\n",
      "  Sorted 16700/25572 files\n",
      "  Sorted 16800/25572 files\n",
      "  Sorted 16900/25572 files\n",
      "  Sorted 17000/25572 files\n",
      "  Sorted 17100/25572 files\n",
      "  Sorted 17200/25572 files\n",
      "  Sorted 17300/25572 files\n",
      "  Sorted 17400/25572 files\n",
      "  Sorted 17500/25572 files\n",
      "  Sorted 17600/25572 files\n",
      "  Sorted 17700/25572 files\n",
      "  Sorted 17800/25572 files\n",
      "  Sorted 17900/25572 files\n",
      "  Sorted 18000/25572 files\n",
      "  Sorted 18100/25572 files\n",
      "  Sorted 18200/25572 files\n",
      "  Sorted 18300/25572 files\n",
      "  Sorted 18400/25572 files\n",
      "  Sorted 18500/25572 files\n",
      "  Sorted 18600/25572 files\n",
      "  Sorted 18700/25572 files\n",
      "  Sorted 18800/25572 files\n",
      "  Sorted 18900/25572 files\n",
      "  Sorted 19000/25572 files\n",
      "  Sorted 19100/25572 files\n",
      "  Sorted 19200/25572 files\n",
      "  Sorted 19300/25572 files\n",
      "  Sorted 19400/25572 files\n",
      "  Sorted 19500/25572 files\n",
      "  Sorted 19600/25572 files\n",
      "  Sorted 19700/25572 files\n",
      "  Sorted 19800/25572 files\n",
      "  Sorted 19900/25572 files\n",
      "  Sorted 20000/25572 files\n",
      "  Sorted 20100/25572 files\n",
      "  Sorted 20200/25572 files\n",
      "  Sorted 20300/25572 files\n",
      "  Sorted 20400/25572 files\n",
      "  Sorted 20500/25572 files\n",
      "  Sorted 20600/25572 files\n",
      "  Sorted 20700/25572 files\n",
      "  Sorted 20800/25572 files\n",
      "  Sorted 20900/25572 files\n",
      "  Sorted 21000/25572 files\n",
      "  Sorted 21100/25572 files\n",
      "  Sorted 21200/25572 files\n",
      "  Sorted 21300/25572 files\n",
      "  Sorted 21400/25572 files\n",
      "  Sorted 21500/25572 files\n",
      "  Sorted 21600/25572 files\n",
      "  Sorted 21700/25572 files\n",
      "  Sorted 21800/25572 files\n",
      "  Sorted 21900/25572 files\n",
      "  Sorted 22000/25572 files\n",
      "  Sorted 22100/25572 files\n",
      "  Sorted 22200/25572 files\n",
      "  Sorted 22300/25572 files\n",
      "  Sorted 22400/25572 files\n",
      "  Sorted 22500/25572 files\n",
      "  Sorted 22600/25572 files\n",
      "  Sorted 22700/25572 files\n",
      "  Sorted 22800/25572 files\n",
      "  Sorted 22900/25572 files\n",
      "  Sorted 23000/25572 files\n",
      "  Sorted 23100/25572 files\n",
      "  Sorted 23200/25572 files\n",
      "  Sorted 23300/25572 files\n",
      "  Sorted 23400/25572 files\n",
      "  Sorted 23500/25572 files\n",
      "  Sorted 23600/25572 files\n",
      "  Sorted 23700/25572 files\n",
      "  Sorted 23800/25572 files\n",
      "  Sorted 23900/25572 files\n",
      "  Sorted 24000/25572 files\n",
      "  Sorted 24100/25572 files\n",
      "  Sorted 24200/25572 files\n",
      "  Sorted 24300/25572 files\n",
      "  Sorted 24400/25572 files\n",
      "  Sorted 24500/25572 files\n",
      "  Sorted 24600/25572 files\n",
      "  Sorted 24700/25572 files\n",
      "  Sorted 24800/25572 files\n",
      "  Sorted 24900/25572 files\n",
      "  Sorted 25000/25572 files\n",
      "  Sorted 25100/25572 files\n",
      "  Sorted 25200/25572 files\n",
      "  Sorted 25300/25572 files\n",
      "  Sorted 25400/25572 files\n",
      "  Sorted 25500/25572 files\n",
      "\n",
      "25572 member files created\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:11:23.659708Z",
     "start_time": "2025-11-11T01:11:07.614023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "\n",
    "# Configuration\n",
    "BY_MEMBER_DIR = '../../data/by_member'\n",
    "n = 10  # Threshold\n",
    "\n",
    "# Get all member files and count transactions\n",
    "member_files = glob(os.path.join(BY_MEMBER_DIR, 'member_*.csv'))\n",
    "counts = [len(pd.read_csv(f)) for f in member_files]\n",
    "\n",
    "# Calculate statistics\n",
    "total_count = len(counts)\n",
    "above_n = sum(1 for c in counts if c >= n)\n",
    "below_n = total_count - above_n\n",
    "above_ratio = (above_n / total_count) * 100\n",
    "below_ratio = (below_n / total_count) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Threshold set to: {n}\")\n",
    "print(f\"Records >= {n}: {above_n:,} ({above_ratio:.2f}%)\")\n",
    "print(f\"Records < {n}: {below_n:,} ({below_ratio:.2f}%)\")"
   ],
   "id": "5f78f9336e45ffef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to: 10\n",
      "Records >= 10: 16,607 (64.94%)\n",
      "Records < 10: 8,965 (35.06%)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage 2: Fraud detection and matching\n",
    "above minimum history length filter"
   ],
   "id": "ec2b1ea69aab8d16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:11:58.262577Z",
     "start_time": "2025-11-11T01:11:27.303274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stage 2: Fraud detection with minimum history length filter\n",
    "min_history_length = 10\n",
    "stats = fr.run_stage2(min_history_length)"
   ],
   "id": "cb6b733ec61d5e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: FRAUD DETECTION\n",
      "============================================================\n",
      "Input: ../../data/by_member\n",
      "Output: ../../data/processed\n",
      "Min History Length: 10\n",
      "\n",
      "Found 25572 member files\n",
      "Filtering: only processing members with >= 10 transactions\n",
      "  Processed 100/25572 members\n",
      "  Processed 200/25572 members\n",
      "  Processed 300/25572 members\n",
      "  Processed 400/25572 members\n",
      "  Processed 500/25572 members\n",
      "  Processed 600/25572 members\n",
      "  Processed 700/25572 members\n",
      "  Processed 800/25572 members\n",
      "  Processed 900/25572 members\n",
      "  Processed 1000/25572 members\n",
      "  Processed 1100/25572 members\n",
      "  Processed 1200/25572 members\n",
      "  Processed 1300/25572 members\n",
      "  Processed 1400/25572 members\n",
      "  Processed 1500/25572 members\n",
      "  Processed 1600/25572 members\n",
      "  Processed 1700/25572 members\n",
      "  Processed 1800/25572 members\n",
      "  Processed 1900/25572 members\n",
      "  Processed 2000/25572 members\n",
      "  Processed 2100/25572 members\n",
      "  Processed 2200/25572 members\n",
      "  Processed 2300/25572 members\n",
      "  Processed 2400/25572 members\n",
      "  Processed 2500/25572 members\n",
      "  Processed 2600/25572 members\n",
      "  Processed 2700/25572 members\n",
      "  Processed 2800/25572 members\n",
      "  Processed 2900/25572 members\n",
      "  Processed 3000/25572 members\n",
      "  Processed 3100/25572 members\n",
      "  Processed 3200/25572 members\n",
      "  Processed 3300/25572 members\n",
      "  Processed 3400/25572 members\n",
      "  Processed 3500/25572 members\n",
      "  Processed 3600/25572 members\n",
      "  Processed 3700/25572 members\n",
      "  Processed 3800/25572 members\n",
      "  Processed 3900/25572 members\n",
      "  Processed 4000/25572 members\n",
      "  Processed 4100/25572 members\n",
      "  Processed 4200/25572 members\n",
      "  Processed 4300/25572 members\n",
      "  Processed 4400/25572 members\n",
      "  Processed 4500/25572 members\n",
      "  Processed 4600/25572 members\n",
      "  Processed 4700/25572 members\n",
      "  Processed 4800/25572 members\n",
      "  Processed 4900/25572 members\n",
      "  Processed 5000/25572 members\n",
      "  Processed 5100/25572 members\n",
      "  Processed 5200/25572 members\n",
      "  Processed 5300/25572 members\n",
      "  Processed 5400/25572 members\n",
      "  Processed 5500/25572 members\n",
      "  Processed 5600/25572 members\n",
      "  Processed 5700/25572 members\n",
      "  Processed 5800/25572 members\n",
      "  Processed 5900/25572 members\n",
      "  Processed 6000/25572 members\n",
      "  Processed 6100/25572 members\n",
      "  Processed 6200/25572 members\n",
      "  Processed 6300/25572 members\n",
      "  Processed 6400/25572 members\n",
      "  Processed 6500/25572 members\n",
      "  Processed 6600/25572 members\n",
      "  Processed 6700/25572 members\n",
      "  Processed 6800/25572 members\n",
      "  Processed 6900/25572 members\n",
      "  Processed 7000/25572 members\n",
      "  Processed 7100/25572 members\n",
      "  Processed 7200/25572 members\n",
      "  Processed 7300/25572 members\n",
      "  Processed 7400/25572 members\n",
      "  Processed 7500/25572 members\n",
      "  Processed 7600/25572 members\n",
      "  Processed 7700/25572 members\n",
      "  Processed 7800/25572 members\n",
      "  Processed 7900/25572 members\n",
      "  Processed 8000/25572 members\n",
      "  Processed 8100/25572 members\n",
      "  Processed 8200/25572 members\n",
      "  Processed 8300/25572 members\n",
      "  Processed 8400/25572 members\n",
      "  Processed 8500/25572 members\n",
      "  Processed 8600/25572 members\n",
      "  Processed 8700/25572 members\n",
      "  Processed 8800/25572 members\n",
      "  Processed 8900/25572 members\n",
      "  Processed 9000/25572 members\n",
      "  Processed 9100/25572 members\n",
      "  Processed 9200/25572 members\n",
      "  Processed 9300/25572 members\n",
      "  Processed 9400/25572 members\n",
      "  Processed 9500/25572 members\n",
      "  Processed 9600/25572 members\n",
      "  Processed 9700/25572 members\n",
      "  Processed 9800/25572 members\n",
      "  Processed 9900/25572 members\n",
      "  Processed 10000/25572 members\n",
      "  Processed 10100/25572 members\n",
      "  Processed 10200/25572 members\n",
      "  Processed 10300/25572 members\n",
      "  Processed 10400/25572 members\n",
      "  Processed 10500/25572 members\n",
      "  Processed 10600/25572 members\n",
      "  Processed 10700/25572 members\n",
      "  Processed 10800/25572 members\n",
      "  Processed 10900/25572 members\n",
      "  Processed 11000/25572 members\n",
      "  Processed 11100/25572 members\n",
      "  Processed 11200/25572 members\n",
      "  Processed 11300/25572 members\n",
      "  Processed 11400/25572 members\n",
      "  Processed 11500/25572 members\n",
      "  Processed 11600/25572 members\n",
      "  Processed 11700/25572 members\n",
      "  Processed 11800/25572 members\n",
      "  Processed 11900/25572 members\n",
      "  Processed 12000/25572 members\n",
      "  Processed 12100/25572 members\n",
      "  Processed 12200/25572 members\n",
      "  Processed 12300/25572 members\n",
      "  Processed 12400/25572 members\n",
      "  Processed 12500/25572 members\n",
      "  Processed 12600/25572 members\n",
      "  Processed 12700/25572 members\n",
      "  Processed 12800/25572 members\n",
      "  Processed 12900/25572 members\n",
      "  Processed 13000/25572 members\n",
      "  Processed 13100/25572 members\n",
      "  Processed 13200/25572 members\n",
      "  Processed 13300/25572 members\n",
      "  Processed 13400/25572 members\n",
      "  Processed 13500/25572 members\n",
      "  Processed 13600/25572 members\n",
      "  Processed 13700/25572 members\n",
      "  Processed 13800/25572 members\n",
      "  Processed 13900/25572 members\n",
      "  Processed 14000/25572 members\n",
      "  Processed 14100/25572 members\n",
      "  Processed 14200/25572 members\n",
      "  Processed 14300/25572 members\n",
      "  Processed 14400/25572 members\n",
      "  Processed 14500/25572 members\n",
      "  Processed 14600/25572 members\n",
      "  Processed 14700/25572 members\n",
      "  Processed 14800/25572 members\n",
      "  Processed 14900/25572 members\n",
      "  Processed 15000/25572 members\n",
      "  Processed 15100/25572 members\n",
      "  Processed 15200/25572 members\n",
      "  Processed 15300/25572 members\n",
      "  Processed 15400/25572 members\n",
      "  Processed 15500/25572 members\n",
      "  Processed 15600/25572 members\n",
      "  Processed 15700/25572 members\n",
      "  Processed 15800/25572 members\n",
      "  Processed 15900/25572 members\n",
      "  Processed 16000/25572 members\n",
      "  Processed 16100/25572 members\n",
      "  Processed 16200/25572 members\n",
      "  Processed 16300/25572 members\n",
      "  Processed 16400/25572 members\n",
      "  Processed 16500/25572 members\n",
      "  Processed 16600/25572 members\n",
      "  Processed 16700/25572 members\n",
      "  Processed 16800/25572 members\n",
      "  Processed 16900/25572 members\n",
      "  Processed 17000/25572 members\n",
      "  Processed 17100/25572 members\n",
      "  Processed 17200/25572 members\n",
      "  Processed 17300/25572 members\n",
      "  Processed 17400/25572 members\n",
      "  Processed 17500/25572 members\n",
      "  Processed 17600/25572 members\n",
      "  Processed 17700/25572 members\n",
      "  Processed 17800/25572 members\n",
      "  Processed 17900/25572 members\n",
      "  Processed 18000/25572 members\n",
      "  Processed 18100/25572 members\n",
      "  Processed 18200/25572 members\n",
      "  Processed 18300/25572 members\n",
      "  Processed 18400/25572 members\n",
      "  Processed 18500/25572 members\n",
      "  Processed 18600/25572 members\n",
      "  Processed 18700/25572 members\n",
      "  Processed 18800/25572 members\n",
      "  Processed 18900/25572 members\n",
      "  Processed 19000/25572 members\n",
      "  Processed 19100/25572 members\n",
      "  Processed 19200/25572 members\n",
      "  Processed 19300/25572 members\n",
      "  Processed 19400/25572 members\n",
      "  Processed 19500/25572 members\n",
      "  Processed 19600/25572 members\n",
      "  Processed 19700/25572 members\n",
      "  Processed 19800/25572 members\n",
      "  Processed 19900/25572 members\n",
      "  Processed 20000/25572 members\n",
      "  Processed 20100/25572 members\n",
      "  Processed 20200/25572 members\n",
      "  Processed 20300/25572 members\n",
      "  Processed 20400/25572 members\n",
      "  Processed 20500/25572 members\n",
      "  Processed 20600/25572 members\n",
      "  Processed 20700/25572 members\n",
      "  Processed 20800/25572 members\n",
      "  Processed 20900/25572 members\n",
      "  Processed 21000/25572 members\n",
      "  Processed 21100/25572 members\n",
      "  Processed 21200/25572 members\n",
      "  Processed 21300/25572 members\n",
      "  Processed 21400/25572 members\n",
      "  Processed 21500/25572 members\n",
      "  Processed 21600/25572 members\n",
      "  Processed 21700/25572 members\n",
      "  Processed 21800/25572 members\n",
      "  Processed 21900/25572 members\n",
      "  Processed 22000/25572 members\n",
      "  Processed 22100/25572 members\n",
      "  Processed 22200/25572 members\n",
      "  Processed 22300/25572 members\n",
      "  Processed 22400/25572 members\n",
      "  Processed 22500/25572 members\n",
      "  Processed 22600/25572 members\n",
      "  Processed 22700/25572 members\n",
      "  Processed 22800/25572 members\n",
      "  Processed 22900/25572 members\n",
      "  Processed 23000/25572 members\n",
      "  Processed 23100/25572 members\n",
      "  Processed 23200/25572 members\n",
      "  Processed 23300/25572 members\n",
      "  Processed 23400/25572 members\n",
      "  Processed 23500/25572 members\n",
      "  Processed 23600/25572 members\n",
      "  Processed 23700/25572 members\n",
      "  Processed 23800/25572 members\n",
      "  Processed 23900/25572 members\n",
      "  Processed 24000/25572 members\n",
      "  Processed 24100/25572 members\n",
      "  Processed 24200/25572 members\n",
      "  Processed 24300/25572 members\n",
      "  Processed 24400/25572 members\n",
      "  Processed 24500/25572 members\n",
      "  Processed 24600/25572 members\n",
      "  Processed 24700/25572 members\n",
      "  Processed 24800/25572 members\n",
      "  Processed 24900/25572 members\n",
      "  Processed 25000/25572 members\n",
      "  Processed 25100/25572 members\n",
      "  Processed 25200/25572 members\n",
      "  Processed 25300/25572 members\n",
      "  Processed 25400/25572 members\n",
      "  Processed 25500/25572 members\n",
      "Summary saved to: ../../data/processed/member_summary.csv\n",
      "\n",
      "Processing Summary:\n",
      "  Total Processed: 16607\n",
      "  Skipped (< 10 txns): 8965\n",
      "  No Fraud: 16435\n",
      "  Matched: 145\n",
      "  Unmatched: 27\n",
      "\n",
      "============================================================\n",
      "COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Feature Engineering: Description Encoding and Clustering\n\nThis stage performs advanced feature engineering on transaction descriptions:\n\n1. **BERT Encoding**: Use BERT-tiny model to encode \"Transaction Description\" text into embeddings\n2. **Dimensionality Reduction**: Apply PCA to reduce embedding dimensions (default: 20D)\n3. **Automatic Clustering**: Find optimal cluster count (k) via heuristic search and cluster with MiniBatchKMeans\n4. **Add Cluster ID**: Append `cluster_id` column to each CSV file\n\n**Input**: Processed member files from `../../data/processed/[matched|unmatched|no_fraud]/`  \n**Output**: Clustered files saved to `../../data/clustered_out/[matched|unmatched|no_fraud]/`\n\n**Note**: This step requires GPU/CPU compute and may take significant time depending on data size.",
   "id": "e268885f085a0a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T01:19:52.770766Z",
     "start_time": "2025-11-11T01:19:51.675851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run Stage 3: Description Encoding and Clustering\n",
    "outputs = fe.main()"
   ],
   "id": "58b1d5fb190b6804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 3: DESCRIPTION ENCODING AND CLUSTERING\n",
      "============================================================\n",
      "Input: ../../data/processed\n",
      "Model: prajjwal1/bert-tiny\n",
      "Text Column: Transaction Description\n",
      "PCA Dimensions: 20\n",
      "Cluster Range: 10-60 (step 10)\n",
      "\n",
      "[Found] 16607 CSV files total.\n",
      "[Device] Using mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 1it [00:00, 100.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PCA] Reducing to 20 dimensions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=25 should be >= n_clusters=60.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run Stage 3: Description Encoding and Clustering\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/03_feature_engineering/03_feature_engineering.py:127\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmain\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    122\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;124;03m    Main entry point for the feature engineering pipeline.\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \n\u001B[1;32m    125\u001B[0m \u001B[38;5;124;03m    Currently runs the description encoding and clustering pipeline.\u001B[39;00m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 127\u001B[0m     \u001B[43mrun_stage3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/03_feature_engineering/03_feature_engineering.py:96\u001B[0m, in \u001B[0;36mrun_stage3\u001B[0;34m(processed_dir, model_name, text_column, batch_size, max_length, pca_dim, min_k, max_k, k_step, sample_size, cluster_batch_size, random_state, verbose)\u001B[0m\n\u001B[1;32m     93\u001B[0m spec\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(desc_encoder)\n\u001B[1;32m     95\u001B[0m \u001B[38;5;66;03m# Call the run_pipeline function with configuration\u001B[39;00m\n\u001B[0;32m---> 96\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdesc_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_pipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessed_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     99\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_column\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_column\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    102\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpca_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpca_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmin_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcluster_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m60\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/03_feature_engineering/03b_description_encoding/description_encoder.py:197\u001B[0m, in \u001B[0;36mrun_pipeline\u001B[0;34m(raw_root, model_name, text_column, batch_size, max_length, pca_dim, min_k, max_k, k_step, sample_size, cluster_batch_size, random_state, cluster_count)\u001B[0m\n\u001B[1;32m    194\u001B[0m outputs: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m csv_path \u001B[38;5;129;01min\u001B[39;00m all_csvs:\n\u001B[0;32m--> 197\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcsv_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_column\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_column\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpca_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpca_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmin_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk_step\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcluster_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcluster_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcluster_count\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result:\n\u001B[1;32m    213\u001B[0m         outputs\u001B[38;5;241m.\u001B[39mappend(result)\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/src/data_preprocess/03_feature_engineering/03b_description_encoding/description_encoder.py:162\u001B[0m, in \u001B[0;36mprocess_csv\u001B[0;34m(file_path, text_column, model_name, batch_size, max_length, pca_dim, min_k, max_k, k_step, sample_size, cluster_batch_size, random_state, cluster_count)\u001B[0m\n\u001B[1;32m    154\u001B[0m effective_k \u001B[38;5;241m=\u001B[39m cluster_count \u001B[38;5;129;01mor\u001B[39;00m DEFAULT_CLUSTER_COUNT\n\u001B[1;32m    156\u001B[0m km \u001B[38;5;241m=\u001B[39m MiniBatchKMeans(\n\u001B[1;32m    157\u001B[0m     n_clusters\u001B[38;5;241m=\u001B[39meffective_k,\n\u001B[1;32m    158\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n\u001B[1;32m    159\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mcluster_batch_size,\n\u001B[1;32m    160\u001B[0m     n_init\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    161\u001B[0m )\n\u001B[0;32m--> 162\u001B[0m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcluster_id\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mkm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduced\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m output_path \u001B[38;5;241m=\u001B[39m _resolve_output_path(file_path)\n\u001B[1;32m    165\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(output_path), exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1064\u001B[0m, in \u001B[0;36m_BaseKMeans.fit_predict\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfit_predict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001B[39;00m\n\u001B[1;32m   1043\u001B[0m \n\u001B[1;32m   1044\u001B[0m \u001B[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1062\u001B[0m \u001B[38;5;124;03m        Index of the cluster each sample belongs to.\u001B[39;00m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1064\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlabels_\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:2079\u001B[0m, in \u001B[0;36mMiniBatchKMeans.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   2044\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute the centroids on X by chunking it into mini-batches.\u001B[39;00m\n\u001B[1;32m   2045\u001B[0m \n\u001B[1;32m   2046\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2068\u001B[0m \u001B[38;5;124;03m    Fitted estimator.\u001B[39;00m\n\u001B[1;32m   2069\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2070\u001B[0m X \u001B[38;5;241m=\u001B[39m validate_data(\n\u001B[1;32m   2071\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   2072\u001B[0m     X,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2076\u001B[0m     accept_large_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   2077\u001B[0m )\n\u001B[0;32m-> 2079\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_params_vs_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2080\u001B[0m random_state \u001B[38;5;241m=\u001B[39m check_random_state(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state)\n\u001B[1;32m   2081\u001B[0m sample_weight \u001B[38;5;241m=\u001B[39m _check_sample_weight(sample_weight, X, dtype\u001B[38;5;241m=\u001B[39mX\u001B[38;5;241m.\u001B[39mdtype)\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1920\u001B[0m, in \u001B[0;36mMiniBatchKMeans._check_params_vs_input\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m   1919\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_check_params_vs_input\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m-> 1920\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_params_vs_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_n_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1922\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   1924\u001B[0m     \u001B[38;5;66;03m# init_size\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:871\u001B[0m, in \u001B[0;36m_BaseKMeans._check_params_vs_input\u001B[0;34m(self, X, default_n_init)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_check_params_vs_input\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, default_n_init\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# n_clusters\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters:\n\u001B[0;32m--> 871\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    872\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m should be >= n_clusters=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_clusters\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    873\u001B[0m         )\n\u001B[1;32m    875\u001B[0m     \u001B[38;5;66;03m# tol\u001B[39;00m\n\u001B[1;32m    876\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tol \u001B[38;5;241m=\u001B[39m _tolerance(X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtol)\n",
      "\u001B[0;31mValueError\u001B[0m: n_samples=25 should be >= n_clusters=60."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "mi1mcm0tomk",
   "source": "### Results Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "eygq4e6c2ew",
   "source": "from glob import glob\nimport os\n\n# Check output files\nCLUSTERED_DIR = '../../data/clustered_out'\nsubfolders = ['matched', 'unmatched', 'no_fraud']\n\nprint(\"=\" * 60)\nprint(\"STAGE 3 OUTPUT SUMMARY\")\nprint(\"=\" * 60)\n\ntotal_files = 0\nstats_list = []\n\nfor subfolder in subfolders:\n    subfolder_path = os.path.join(CLUSTERED_DIR, subfolder)\n    \n    if not os.path.exists(subfolder_path):\n        print(f\"\\n{subfolder}: Directory not found\")\n        continue\n    \n    csv_files = glob(os.path.join(subfolder_path, 'member_*.csv'))\n    \n    if csv_files:\n        # Sample one file to check cluster_id column\n        sample_df = pd.read_csv(csv_files[0])\n        has_cluster_id = 'cluster_id' in sample_df.columns\n        \n        if has_cluster_id:\n            # Get cluster statistics from sample\n            cluster_counts = sample_df['cluster_id'].value_counts().sort_index()\n            num_clusters = len(cluster_counts)\n        else:\n            num_clusters = \"N/A\"\n        \n        stats_list.append({\n            'Category': subfolder,\n            'Files': len(csv_files),\n            'Has cluster_id': '✓' if has_cluster_id else '✗',\n            'Clusters (sample)': num_clusters if has_cluster_id else \"N/A\"\n        })\n        \n        total_files += len(csv_files)\n        print(f\"\\n{subfolder}:\")\n        print(f\"  Files processed: {len(csv_files)}\")\n        if has_cluster_id:\n            print(f\"  Cluster ID column: ✓\")\n            print(f\"  Clusters in sample file: {num_clusters}\")\n    else:\n        print(f\"\\n{subfolder}: No files found\")\n\nprint(f\"\\n{'=' * 60}\")\nprint(f\"Total files with cluster_id: {total_files}\")\nprint(f\"{'=' * 60}\")\n\n# Display summary table\nif stats_list:\n    df_summary = pd.DataFrame(stats_list)\n    display(df_summary)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5o3ixzv5ahs",
   "source": "---\n\n**Next Steps**: After Stage 3 completes, you can proceed to Stage 4 (Feature Encoding) using `04_encoding/encoding.py` to encode categorical features and prepare the final dataset for model training.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
