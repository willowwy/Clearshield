{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for Data Preprocessing",
   "id": "d7225e284319416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Import and Configuration",
   "id": "1c512e97635def66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:48:53.389968Z",
     "start_time": "2025-11-12T04:48:53.379543Z"
    }
   },
   "cell_type": "code",
   "source": "import sys\nimport os\nimport pandas as pd\nimport importlib.util\n\n# Import data cleaning module\nsys.path.insert(0, os.path.abspath('./01_data_cleaning'))\nspec = importlib.util.spec_from_file_location(\n    \"data_cleaning\",\n    \"./01_data_cleaning/01_data_cleaning.py\"\n)\ndc = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(dc)\n\n# Import feature engineering module\nspec = importlib.util.spec_from_file_location(\n    \"feature_engineering\",\n    \"02_feature_engineering/02_feature_engineering.py\"\n)\nfe = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fe)\n\n# Import fraud relabeling module\nspec = importlib.util.spec_from_file_location(\n    \"fraud_relabeling\",\n    \"03_fraud_relabeling/03_fraud_relabeling.py\"\n)\nfr = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fr)\n\n# Import encoding module\nspec = importlib.util.spec_from_file_location(\n    \"encoding\",\n    \"04_encoding/04_encoding.py\"\n)\nenc = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(enc)\n\n# Configure data cleaning\ndc.ENABLE_RENAMING = True\ndc.RAW_DIR = '../../data/raw'\ndc.CLEANED_DIR = '../../data/cleaned'\n\n# Configure feature engineering\nfe.PROCESSED_DIR = '../../data/cleaned'  # Read from cleaned data\nfe.MODEL_NAME = 'prajjwal1/bert-tiny'\nfe.TEXT_COLUMN = 'Transaction Description'\nfe.BATCH_SIZE = 64\nfe.MAX_LENGTH = 64\nfe.PCA_DIM = 20\nfe.MIN_K = 10\nfe.MAX_K = 60\nfe.K_STEP = 10\nfe.SAMPLE_SIZE = 10000\nfe.CLUSTER_BATCH_SIZE = 4096\nfe.RANDOM_STATE = 42\n\n# Configure fraud relabeling\nfr.INPUT_DIR = '../../data/clustered_out'  # Read from feature engineering output\nfr.OUTPUT_MEMBER_DIR = '../../data/by_member'\nfr.OUTPUT_PROCESSED_DIR = '../../data/processed'\nfr.CHUNKSIZE = 50000\n\n# Configure encoding\nenc.PROCESSED_DIR = '../../data/processed'  # Read from fraud relabeling output\nenc.OUTPUT_DIR = '../../data/final'\nenc.CONFIG_PATH = '../../config/tokenize_dict.json'\n\n# Create directories if they don't exist\ndirectories = [\n    dc.RAW_DIR,\n    dc.CLEANED_DIR,\n    fr.INPUT_DIR,\n    fr.OUTPUT_MEMBER_DIR,\n    fr.OUTPUT_PROCESSED_DIR,\n    enc.OUTPUT_DIR\n]\n\nfor directory in directories:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        print(f\"Created directory: {directory}\")",
   "id": "179ab9ab779e8ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../../data/clustered_out\n",
      "Created directory: ../../data/by_member\n",
      "Created directory: ../../data/processed\n",
      "Created directory: ../../data/final\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data Cleaning\n\nThis cell performs the following preprocessing tasks:\n1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n2. Fix comma issues (remove extra commas in field values)\n3. Clean Amount field (remove $ and commas, convert to numeric)\n4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n5. Rename files based on date range (MM-DD-YYYY-MM-DD-YYYY.csv)\n\n**Input**: Raw CSV files from `../../data/raw/`  \n**Output**: Cleaned and renamed CSV files saved to `../../data/cleaned/`",
   "id": "50b577d75aae631e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:48:48.270501Z",
     "start_time": "2025-11-12T04:48:19.285895Z"
    }
   },
   "cell_type": "code",
   "source": "dc.main()",
   "id": "8ec45af1c8067334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ../../data/raw\n",
      "Cleaned: ../../data/cleaned\n",
      "\n",
      "Found 1 CSV files in ../../data/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. Transaction Data 09232025 (5).csv (63.05 MB)\n",
      "\n",
      "============================================================\n",
      "Processing Files...\n",
      "============================================================\n",
      "\n",
      "[1/1] Transaction Data 09232025 (5).csv... Missing:28077, →09-01-2024_to_09-01-2025.csv\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaned File Analysis",
   "id": "c372e04ff766b0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Size & Rows & Date Span",
   "id": "3b312b770f289f51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:53:11.915751Z",
     "start_time": "2025-11-11T21:53:11.443393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "# Load all dataframes\n",
    "dfs = {}\n",
    "for filename in csv_files:\n",
    "    dfs[filename] = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "\n",
    "# Collect stats\n",
    "stats = []\n",
    "for filename, df in dfs.items():\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    min_date = df['Post Date'].min()\n",
    "    max_date = df['Post Date'].max()\n",
    "    fraud_count = ((df['Fraud Adjustment Indicator'].notna()) &\n",
    "                   (df['Fraud Adjustment Indicator'] != '')).sum()\n",
    "\n",
    "    stats.append({\n",
    "        'File': filename,\n",
    "        'Rows': len(df),\n",
    "        'Members': df['Member ID'].nunique(),\n",
    "        'Date From': min_date.strftime('%m/%d/%Y') if pd.notna(min_date) else 'N/A',\n",
    "        'Date To': max_date.strftime('%m/%d/%Y') if pd.notna(max_date) else 'N/A',\n",
    "        'Days': (max_date - min_date).days if pd.notna(min_date) else 0,\n",
    "        'Fraud %': round(fraud_count / len(df) * 100, 4) if len(df) > 0 else 0\n",
    "    })\n",
    "\n",
    "# Display table\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "# Summary\n",
    "all_members = set()\n",
    "for df in dfs.values():\n",
    "    all_members.update(df['Member ID'].dropna())\n",
    "\n",
    "total_fraud = sum([((dfs[f]['Fraud Adjustment Indicator'].notna()) &\n",
    "                    (dfs[f]['Fraud Adjustment Indicator'] != '')).sum()\n",
    "                   for f in dfs.keys()])\n",
    "\n",
    "print(f\"\\nTotal Rows: {df_stats['Rows'].sum():,}\")\n",
    "print(f\"Total Unique Members: {len(all_members):,}\")\n",
    "print(f\"Total Fraud Indicators: {total_fraud:,}\")\n",
    "print(f\"Overall Fraud %: {round(total_fraud / df_stats['Rows'].sum() * 100, 4)}%\")"
   ],
   "id": "d9503066fa80edd1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           File    Rows  Members   Date From     Date To  \\\n",
       "0  09-01-2024_to_09-01-2025.csv  579672     1981  09/01/2024  09/01/2025   \n",
       "\n",
       "   Days  Fraud %  \n",
       "0   365    0.068  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Members</th>\n",
       "      <th>Date From</th>\n",
       "      <th>Date To</th>\n",
       "      <th>Days</th>\n",
       "      <th>Fraud %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09-01-2024_to_09-01-2025.csv</td>\n",
       "      <td>579672</td>\n",
       "      <td>1981</td>\n",
       "      <td>09/01/2024</td>\n",
       "      <td>09/01/2025</td>\n",
       "      <td>365</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Rows: 579,672\n",
      "Total Unique Members: 1,981\n",
      "Total Fraud Indicators: 394\n",
      "Overall Fraud %: 0.068%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Overlapping detection",
   "id": "77b6792970472928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:53:18.186397Z",
     "start_time": "2025-11-11T21:53:17.250559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "print(f\"Loading {len(csv_files)} files...\")\n",
    "\n",
    "# Load files and create row IDs\n",
    "file_rows = {}\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "    row_ids = set(df['Account ID'].astype(str) + '|' +\n",
    "                  df['Member ID'].astype(str) + '|' +\n",
    "                  df['Post Date'].astype(str) + '|' +\n",
    "                  df['Post Time'].astype(str) + '|' +\n",
    "                  df['Amount'].astype(str))\n",
    "    file_rows[filename] = row_ids\n",
    "\n",
    "# Calculate pairwise overlaps\n",
    "results = []\n",
    "for i, file1 in enumerate(csv_files):\n",
    "    for j, file2 in enumerate(csv_files):\n",
    "        if i < j:\n",
    "            overlap = len(file_rows[file1] & file_rows[file2])\n",
    "            pct1 = overlap / len(file_rows[file1]) * 100\n",
    "            pct2 = overlap / len(file_rows[file2]) * 100\n",
    "\n",
    "            results.append({\n",
    "                'File 1': file1,\n",
    "                'File 2': file2,\n",
    "                'Overlap Rows': overlap,\n",
    "                '% of File 1': round(pct1, 1),\n",
    "                '% of File 2': round(pct2, 1)\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display as styled DataFrame\n",
    "display(df_results)"
   ],
   "id": "34a3032caa9d448c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Feature Engineering: Description Encoding and Clustering\n\nThis stage performs advanced feature engineering on transaction descriptions:\n\n1. **BERT Encoding**: Use BERT-tiny model to encode \"Transaction Description\" text into embeddings\n2. **Dimensionality Reduction**: Apply PCA to reduce embedding dimensions (default: 20D)\n3. **Automatic Clustering**: Find optimal cluster count (k) via heuristic search and cluster with MiniBatchKMeans\n4. **Add Cluster ID**: Append `cluster_id` column to each CSV file\n\n**Input**: Cleaned CSV files from `../../data/cleaned/`  \n**Output**: Clustered files saved to `../../data/clustered_out/`\n\n**Note**: This step requires GPU/CPU compute and may take significant time depending on data size.",
   "id": "e268885f085a0a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T04:49:50.883544Z",
     "start_time": "2025-11-12T04:49:05.988133Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = fe.main()",
   "id": "58b1d5fb190b6804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: DESCRIPTION ENCODING AND CLUSTERING\n",
      "============================================================\n",
      "Input: ../../data/cleaned\n",
      "Model: prajjwal1/bert-tiny\n",
      "Text Column: Transaction Description\n",
      "PCA Dimensions: 20\n",
      "Cluster Range: 10-60 (step 10)\n",
      "\n",
      "[Scan] Found 1 CSV file(s) in ../../data/cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Saved 1 clustered file(s) to /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/clustered_out\n",
      "\n",
      "============================================================\n",
      "STAGE 2 COMPLETE\n",
      "============================================================\n",
      "Processed 1 files\n",
      "Output location: ../../data/cleaned\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Fraud Matching and Re-label\nThis cell performs fraud detection in two stages:\n\n1. **Reorganize by Member**: Group all transactions by Member ID into individual files\n2. **Match Fraud Adjustments**: Find and mark original fraudulent transactions for each refund record （>10）\n   - Match by amount and date (extract from description or 30-day range)\n   - Prevent duplicate matching\n   - Categorize as matched/unmatched/no_fraud\n\n**Input**: Clustered files from `../../data/clustered_out/`  \n**Output**: Processed member files saved to `../../data/processed/[matched|unmatched|no_fraud]/`",
   "id": "64b755731928b893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage 3-1: Reorganize transactions by member\n",
    "\n",
    "**Input**: Clustered files from `../../data/clustered_out/`  \n",
    "**Output**: Member-grouped files saved to `../../data/by_member/`"
   ],
   "id": "f60bb8ed29d09f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:02:02.486282Z",
     "start_time": "2025-11-12T00:51:08.335915Z"
    }
   },
   "cell_type": "code",
   "source": "num_members = fr.run_stage1()",
   "id": "7156ce60494636e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1: DATA REORGANIZATION\n",
      "============================================================\n",
      "Input: ../../data/clustered_out\n",
      "Output: ../../data/by_member\n",
      "\n",
      "Found 4 files\n",
      "Processing 1/4: 01-01-2025_to_03-31-2025.csv\n",
      "Processing 2/4: 04-01-2025_to_06-30-2025.csv\n",
      "Processing 3/4: 07-01-2025_to_09-01-2025.csv\n",
      "Processing 4/4: 09-01-2024_to_12-31-2024.csv\n",
      "Created 29656 member files\n",
      "Sorting files...\n",
      "  Sorted 1000/29656 files\n",
      "  Sorted 2000/29656 files\n",
      "  Sorted 3000/29656 files\n",
      "  Sorted 4000/29656 files\n",
      "  Sorted 5000/29656 files\n",
      "  Sorted 6000/29656 files\n",
      "  Sorted 7000/29656 files\n",
      "  Sorted 8000/29656 files\n",
      "  Sorted 9000/29656 files\n",
      "  Sorted 10000/29656 files\n",
      "  Sorted 11000/29656 files\n",
      "  Sorted 12000/29656 files\n",
      "  Sorted 13000/29656 files\n",
      "  Sorted 14000/29656 files\n",
      "  Sorted 15000/29656 files\n",
      "  Sorted 16000/29656 files\n",
      "  Sorted 17000/29656 files\n",
      "  Sorted 18000/29656 files\n",
      "  Sorted 19000/29656 files\n",
      "  Sorted 20000/29656 files\n",
      "  Sorted 21000/29656 files\n",
      "  Sorted 22000/29656 files\n",
      "  Sorted 23000/29656 files\n",
      "  Sorted 24000/29656 files\n",
      "  Sorted 25000/29656 files\n",
      "  Sorted 26000/29656 files\n",
      "  Sorted 27000/29656 files\n",
      "  Sorted 28000/29656 files\n",
      "  Sorted 29000/29656 files\n",
      "\n",
      "29656 member files created\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:02:57.896198Z",
     "start_time": "2025-11-12T01:02:32.052884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "\n",
    "# Configuration\n",
    "BY_MEMBER_DIR = '../../data/by_member'\n",
    "n = 10  # Threshold\n",
    "\n",
    "# Get all member files and count transactions\n",
    "member_files = glob(os.path.join(BY_MEMBER_DIR, 'member_*.csv'))\n",
    "counts = [len(pd.read_csv(f)) for f in member_files]\n",
    "\n",
    "# Calculate statistics\n",
    "total_count = len(counts)\n",
    "above_n = sum(1 for c in counts if c >= n)\n",
    "below_n = total_count - above_n\n",
    "above_ratio = (above_n / total_count) * 100\n",
    "below_ratio = (below_n / total_count) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Threshold set to: {n}\")\n",
    "print(f\"Records >= {n}: {above_n:,} ({above_ratio:.2f}%)\")\n",
    "print(f\"Records < {n}: {below_n:,} ({below_ratio:.2f}%)\")"
   ],
   "id": "5f78f9336e45ffef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to: 10\n",
      "Records >= 10: 23,295 (78.55%)\n",
      "Records < 10: 6,361 (21.45%)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stage 3-2: Fraud detection and matching\n",
    "\n",
    "Filter members with minimum history length (≥10 transactions), then match fraud adjustments to original transactions.\n",
    "\n",
    "**Input**: Member-grouped files from `../../data/by_member/`  \n",
    "**Output**: Processed and categorized files saved to `../../data/processed/[matched|unmatched|no_fraud]/`"
   ],
   "id": "ec2b1ea69aab8d16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:04:12.268369Z",
     "start_time": "2025-11-12T01:03:02.500967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stage 2: Fraud detection with minimum history length filter\n",
    "min_history_length = 10\n",
    "stats = fr.run_stage2(min_history_length)"
   ],
   "id": "cb6b733ec61d5e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: FRAUD DETECTION\n",
      "============================================================\n",
      "Input: ../../data/by_member\n",
      "Output: ../../data/processed\n",
      "Min History Length: 10\n",
      "\n",
      "Found 29656 member files\n",
      "Filtering: only processing members with >= 10 transactions\n",
      "  Processed 1000/29656 members\n",
      "  Processed 2000/29656 members\n",
      "  Processed 3000/29656 members\n",
      "  Processed 4000/29656 members\n",
      "  Processed 5000/29656 members\n",
      "  Processed 6000/29656 members\n",
      "  Processed 7000/29656 members\n",
      "  Processed 8000/29656 members\n",
      "  Processed 9000/29656 members\n",
      "  Processed 10000/29656 members\n",
      "  Processed 11000/29656 members\n",
      "  Processed 12000/29656 members\n",
      "  Processed 13000/29656 members\n",
      "  Processed 14000/29656 members\n",
      "  Processed 15000/29656 members\n",
      "  Processed 16000/29656 members\n",
      "  Processed 17000/29656 members\n",
      "  Processed 18000/29656 members\n",
      "  Processed 19000/29656 members\n",
      "  Processed 20000/29656 members\n",
      "  Processed 21000/29656 members\n",
      "  Processed 22000/29656 members\n",
      "  Processed 23000/29656 members\n",
      "  Processed 24000/29656 members\n",
      "  Processed 25000/29656 members\n",
      "  Processed 26000/29656 members\n",
      "  Processed 27000/29656 members\n",
      "  Processed 28000/29656 members\n",
      "  Processed 29000/29656 members\n",
      "Summary saved to: ../../data/processed/member_summary.csv\n",
      "\n",
      "Processing Summary:\n",
      "  Total Processed: 23295\n",
      "  Skipped (< 10 txns): 6361\n",
      "  No Fraud: 22673\n",
      "  Matched: 544\n",
      "  Unmatched: 78\n",
      "\n",
      "============================================================\n",
      "COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "ou5w7jmthd8",
   "source": "## 4. Feature Encoding\n\nThis stage encodes categorical features and prepares the final dataset for model training:\n\n1. **Remove ID Columns**: Delete Account ID and Member ID\n2. **Encode Categorical Features**: Convert categorical columns to numeric using predefined dictionary\n   - Account Type, Action Type, Source Type, Product ID\n3. **Parse Time Features**: Convert Post Time to decimal hours\n4. **Convert Date Features**: Parse Post Date and Account Open Date\n5. **Clean Up**: Remove text columns (Transaction Description, Fraud Adjustment Indicator)\n\n**Input**: Processed member files from `../../data/processed/[matched|unmatched|no_fraud]/`  \n**Output**: Final encoded files saved to `../../data/final/[matched|unmatched|no_fraud]/`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cmq993u1rh",
   "source": "total_processed = enc.encode_features(enc.PROCESSED_DIR, enc.OUTPUT_DIR, enc.CONFIG_PATH)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T01:35:54.780503Z",
     "start_time": "2025-11-12T01:28:24.508706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENCODING\n",
      "============================================================\n",
      "Input Dir: ../../data/processed\n",
      "Output Dir: ../../data/final\n",
      "Config Path: ../../config/tokenize_dict.json\n",
      "\n",
      "Loaded encoding dictionary with 4 features\n",
      "\n",
      "matched: Found 544 files\n",
      "  Processed 100/544 files\n",
      "  Processed 200/544 files\n",
      "  Processed 300/544 files\n",
      "  Processed 400/544 files\n",
      "  Processed 500/544 files\n",
      "  matched: Encoded 544/544 files\n",
      "\n",
      "unmatched: Found 78 files\n",
      "  unmatched: Encoded 78/78 files\n",
      "\n",
      "no_fraud: Found 22673 files\n",
      "  Processed 100/22673 files\n",
      "  Processed 200/22673 files\n",
      "  Processed 300/22673 files\n",
      "  Processed 400/22673 files\n",
      "  Processed 500/22673 files\n",
      "  Processed 600/22673 files\n",
      "  Processed 700/22673 files\n",
      "  Processed 800/22673 files\n",
      "  Processed 900/22673 files\n",
      "  Processed 1000/22673 files\n",
      "  Processed 1100/22673 files\n",
      "  Processed 1200/22673 files\n",
      "  Processed 1300/22673 files\n",
      "  Processed 1400/22673 files\n",
      "  Processed 1500/22673 files\n",
      "  Processed 1600/22673 files\n",
      "  Processed 1700/22673 files\n",
      "  Processed 1800/22673 files\n",
      "  Processed 1900/22673 files\n",
      "  Processed 2000/22673 files\n",
      "  Processed 2100/22673 files\n",
      "  Processed 2200/22673 files\n",
      "  Processed 2300/22673 files\n",
      "  Processed 2400/22673 files\n",
      "  Processed 2500/22673 files\n",
      "  Processed 2600/22673 files\n",
      "  Processed 2700/22673 files\n",
      "  Processed 2800/22673 files\n",
      "  Processed 2900/22673 files\n",
      "  Processed 3000/22673 files\n",
      "  Processed 3100/22673 files\n",
      "  Processed 3200/22673 files\n",
      "  Processed 3300/22673 files\n",
      "  Processed 3400/22673 files\n",
      "  Processed 3500/22673 files\n",
      "  Processed 3600/22673 files\n",
      "  Processed 3700/22673 files\n",
      "  Processed 3800/22673 files\n",
      "  Processed 3900/22673 files\n",
      "  Processed 4000/22673 files\n",
      "  Processed 4100/22673 files\n",
      "  Processed 4200/22673 files\n",
      "  Processed 4300/22673 files\n",
      "  Processed 4400/22673 files\n",
      "  Processed 4500/22673 files\n",
      "  Processed 4600/22673 files\n",
      "  Processed 4700/22673 files\n",
      "  Processed 4800/22673 files\n",
      "  Processed 4900/22673 files\n",
      "  Processed 5000/22673 files\n",
      "  Processed 5100/22673 files\n",
      "  Processed 5200/22673 files\n",
      "  Processed 5300/22673 files\n",
      "  Processed 5400/22673 files\n",
      "  Processed 5500/22673 files\n",
      "  Processed 5600/22673 files\n",
      "  Processed 5700/22673 files\n",
      "  Processed 5800/22673 files\n",
      "  Processed 5900/22673 files\n",
      "  Processed 6000/22673 files\n",
      "  Processed 6100/22673 files\n",
      "  Processed 6200/22673 files\n",
      "  Processed 6300/22673 files\n",
      "  Processed 6400/22673 files\n",
      "  Processed 6500/22673 files\n",
      "  Processed 6600/22673 files\n",
      "  Processed 6700/22673 files\n",
      "  Processed 6800/22673 files\n",
      "  Processed 6900/22673 files\n",
      "  Processed 7000/22673 files\n",
      "  Processed 7100/22673 files\n",
      "  Processed 7200/22673 files\n",
      "  Processed 7300/22673 files\n",
      "  Processed 7400/22673 files\n",
      "  Processed 7500/22673 files\n",
      "  Processed 7600/22673 files\n",
      "  Processed 7700/22673 files\n",
      "  Processed 7800/22673 files\n",
      "  Processed 7900/22673 files\n",
      "  Processed 8000/22673 files\n",
      "  Processed 8100/22673 files\n",
      "  Processed 8200/22673 files\n",
      "  Processed 8300/22673 files\n",
      "  Processed 8400/22673 files\n",
      "  Processed 8500/22673 files\n",
      "  Processed 8600/22673 files\n",
      "  Processed 8700/22673 files\n",
      "  Processed 8800/22673 files\n",
      "  Processed 8900/22673 files\n",
      "  Processed 9000/22673 files\n",
      "  Processed 9100/22673 files\n",
      "  Processed 9200/22673 files\n",
      "  Processed 9300/22673 files\n",
      "  Processed 9400/22673 files\n",
      "  Processed 9500/22673 files\n",
      "  Processed 9600/22673 files\n",
      "  Processed 9700/22673 files\n",
      "  Processed 9800/22673 files\n",
      "  Processed 9900/22673 files\n",
      "  Processed 10000/22673 files\n",
      "  Processed 10100/22673 files\n",
      "  Processed 10200/22673 files\n",
      "  Processed 10300/22673 files\n",
      "  Processed 10400/22673 files\n",
      "  Processed 10500/22673 files\n",
      "  Processed 10600/22673 files\n",
      "  Processed 10700/22673 files\n",
      "  Processed 10800/22673 files\n",
      "  Processed 10900/22673 files\n",
      "  Processed 11000/22673 files\n",
      "  Processed 11100/22673 files\n",
      "  Processed 11200/22673 files\n",
      "  Processed 11300/22673 files\n",
      "  Processed 11400/22673 files\n",
      "  Processed 11500/22673 files\n",
      "  Processed 11600/22673 files\n",
      "  Processed 11700/22673 files\n",
      "  Processed 11800/22673 files\n",
      "  Processed 11900/22673 files\n",
      "  Processed 12000/22673 files\n",
      "  Processed 12100/22673 files\n",
      "  Processed 12200/22673 files\n",
      "  Processed 12300/22673 files\n",
      "  Processed 12400/22673 files\n",
      "  Processed 12500/22673 files\n",
      "  Processed 12600/22673 files\n",
      "  Processed 12700/22673 files\n",
      "  Processed 12800/22673 files\n",
      "  Processed 12900/22673 files\n",
      "  Processed 13000/22673 files\n",
      "  Processed 13100/22673 files\n",
      "  Processed 13200/22673 files\n",
      "  Processed 13300/22673 files\n",
      "  Processed 13400/22673 files\n",
      "  Processed 13500/22673 files\n",
      "  Processed 13600/22673 files\n",
      "  Processed 13700/22673 files\n",
      "  Processed 13800/22673 files\n",
      "  Processed 13900/22673 files\n",
      "  Processed 14000/22673 files\n",
      "  Processed 14100/22673 files\n",
      "  Processed 14200/22673 files\n",
      "  Processed 14300/22673 files\n",
      "  Processed 14400/22673 files\n",
      "  Processed 14500/22673 files\n",
      "  Processed 14600/22673 files\n",
      "  Processed 14700/22673 files\n",
      "  Processed 14800/22673 files\n",
      "  Processed 14900/22673 files\n",
      "  Processed 15000/22673 files\n",
      "  Processed 15100/22673 files\n",
      "  Processed 15200/22673 files\n",
      "  Processed 15300/22673 files\n",
      "  Processed 15400/22673 files\n",
      "  Processed 15500/22673 files\n",
      "  Processed 15600/22673 files\n",
      "  Processed 15700/22673 files\n",
      "  Processed 15800/22673 files\n",
      "  Processed 15900/22673 files\n",
      "  Processed 16000/22673 files\n",
      "  Processed 16100/22673 files\n",
      "  Processed 16200/22673 files\n",
      "  Processed 16300/22673 files\n",
      "  Processed 16400/22673 files\n",
      "  Processed 16500/22673 files\n",
      "  Processed 16600/22673 files\n",
      "  Processed 16700/22673 files\n",
      "  Processed 16800/22673 files\n",
      "  Processed 16900/22673 files\n",
      "  Processed 17000/22673 files\n",
      "  Processed 17100/22673 files\n",
      "  Processed 17200/22673 files\n",
      "  Processed 17300/22673 files\n",
      "  Processed 17400/22673 files\n",
      "  Processed 17500/22673 files\n",
      "  Processed 17600/22673 files\n",
      "  Processed 17700/22673 files\n",
      "  Processed 17800/22673 files\n",
      "  Processed 17900/22673 files\n",
      "  Processed 18000/22673 files\n",
      "  Processed 18100/22673 files\n",
      "  Processed 18200/22673 files\n",
      "  Processed 18300/22673 files\n",
      "  Processed 18400/22673 files\n",
      "  Processed 18500/22673 files\n",
      "  Processed 18600/22673 files\n",
      "  Processed 18700/22673 files\n",
      "  Processed 18800/22673 files\n",
      "  Processed 18900/22673 files\n",
      "  Processed 19000/22673 files\n",
      "  Processed 19100/22673 files\n",
      "  Processed 19200/22673 files\n",
      "  Processed 19300/22673 files\n",
      "  Processed 19400/22673 files\n",
      "  Processed 19500/22673 files\n",
      "  Processed 19600/22673 files\n",
      "  Processed 19700/22673 files\n",
      "  Processed 19800/22673 files\n",
      "  Processed 19900/22673 files\n",
      "  Processed 20000/22673 files\n",
      "  Processed 20100/22673 files\n",
      "  Processed 20200/22673 files\n",
      "  Processed 20300/22673 files\n",
      "  Processed 20400/22673 files\n",
      "  Processed 20500/22673 files\n",
      "  Processed 20600/22673 files\n",
      "  Processed 20700/22673 files\n",
      "  Processed 20800/22673 files\n",
      "  Processed 20900/22673 files\n",
      "  Processed 21000/22673 files\n",
      "  Processed 21100/22673 files\n",
      "  Processed 21200/22673 files\n",
      "  Processed 21300/22673 files\n",
      "  Processed 21400/22673 files\n",
      "  Processed 21500/22673 files\n",
      "  Processed 21600/22673 files\n",
      "  Processed 21700/22673 files\n",
      "  Processed 21800/22673 files\n",
      "  Processed 21900/22673 files\n",
      "  Processed 22000/22673 files\n",
      "  Processed 22100/22673 files\n",
      "  Processed 22200/22673 files\n",
      "  Processed 22300/22673 files\n",
      "  Processed 22400/22673 files\n",
      "  Processed 22500/22673 files\n",
      "  Processed 22600/22673 files\n",
      "  no_fraud: Encoded 22673/22673 files\n",
      "\n",
      "============================================================\n",
      "Encoding Complete!\n",
      "Total files found: 23295\n",
      "Total files processed: 23295\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "75bd63344823637a",
   "source": "TODO : vulnerability scanner",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Pipeline Complete!\n",
    "\n",
    "The complete data preprocessing pipeline consists of 4 stages:\n",
    "\n",
    "1. **Data Cleaning**: Raw CSV → Cleaned CSV (`data/cleaned/`)\n",
    "2. **Feature Engineering**: Cleaned CSV → Clustered CSV (`data/clustered_out/`)\n",
    "3. **Fraud Matching**: Clustered CSV → Categorized by Member (`data/processed/`)\n",
    "4. **Feature Encoding**: Processed CSV → Final Encoded Dataset (`data/final/`)\n",
    "\n",
    "**Final Output**: `../../data/final/[matched|unmatched|no_fraud]/member_*.csv`\n",
    "\n",
    "These final encoded files are ready for model training!"
   ],
   "id": "5o3ixzv5ahs"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
