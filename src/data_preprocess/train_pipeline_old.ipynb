{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for Data Preprocessing",
   "id": "d7225e284319416"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 0. Import and Configuration",
   "id": "1c512e97635def66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:08:50.201424Z",
     "start_time": "2025-11-20T20:08:50.185552Z"
    }
   },
   "cell_type": "code",
   "source": "import sys\nimport os\nimport pandas as pd\nimport importlib.util\n\n# Import data cleaning module\nsys.path.insert(0, os.path.abspath('./01_data_cleaning'))\nspec = importlib.util.spec_from_file_location(\n    \"data_cleaning\",\n    \"./01_data_cleaning/01_data_cleaning.py\"\n)\ndc = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(dc)\n\n# Import feature engineering module\nspec = importlib.util.spec_from_file_location(\n    \"feature_engineering\",\n    \"02_feature_engineering/02_feature_engineering.py\"\n)\nfe = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fe)\n\n# Import fraud relabeling module\nspec = importlib.util.spec_from_file_location(\n    \"fraud_relabeling\",\n    \"03_fraud_relabeling/03_fraud_relabeling.py\"\n)\nfr = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(fr)\n\n# Import encoding module\nspec = importlib.util.spec_from_file_location(\n    \"encoding\",\n    \"04_encoding/04_encoding.py\"\n)\nenc = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(enc)\n\n# Configure data cleaning\ndc.ENABLE_RENAMING = True\ndc.RAW_DIR = '../../data/train/raw'\ndc.CLEANED_DIR = '../../data/train/cleaned'\n\n# Configure feature engineering\nfe.PROCESSED_DIR = '../../data/train/cleaned'  # Read from cleaned data\nfe.MODEL_NAME = 'prajjwal1/bert-tiny'\nfe.TEXT_COLUMN = 'Transaction Description'\nfe.BATCH_SIZE = 64\nfe.MAX_LENGTH = 64\nfe.PCA_DIM = 20\nfe.MIN_K = 10\nfe.MAX_K = 60\nfe.K_STEP = 10\nfe.SAMPLE_SIZE = 10000\nfe.CLUSTER_BATCH_SIZE = 4096\nfe.RANDOM_STATE = 42\n\n# Configure fraud relabeling\nfr.INPUT_DIR = '../../data/train/clustered_out'  # Read from feature engineering output\nfr.OUTPUT_MEMBER_DIR = '../../data/train/by_member/temp'  # Temporary directory\nfr.OUTPUT_PROCESSED_DIR = '../../data/train/by_member'  # Output directory with matched/unmatched/no_fraud\nfr.CHUNKSIZE = 50000\n\n# Configure encoding\nenc.PROCESSED_DIR = '../../data/train/by_member'  # Read from fraud relabeling output\nenc.OUTPUT_DIR = '../../data/train/final'\nenc.CONFIG_PATH = '../../config/tokenize_dict.json'\n\n# Create directories if they don't exist\ndirectories = [\n    dc.RAW_DIR,\n    dc.CLEANED_DIR,\n    fr.INPUT_DIR,\n    fr.OUTPUT_MEMBER_DIR,\n    enc.OUTPUT_DIR\n]\n\nfor directory in directories:\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        print(f\"Created directory: {directory}\")",
   "id": "179ab9ab779e8ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: ../../data/train/cleaned\n",
      "Created directory: ../../data/train/clustered_out\n",
      "Created directory: ../../data/train/by_member/temp\n",
      "Created directory: ../../data/train/final\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data Cleaning\n\nThis cell performs the following preprocessing tasks:\n1. Standardize headers (e.g., \"AccountID\" → \"Account ID\")\n2. Fix comma issues (remove extra commas in field values)\n3. Clean Amount field (remove $ and commas, convert to numeric)\n4. Fill missing values (Amount→0, others→\"Unknown\", \"null\"→empty)\n5. Rename files based on date range (MM-DD-YYYY-MM-DD-YYYY.csv)\n\n**Input**: Raw CSV files from `../../data/raw/`  \n**Output**: Cleaned and renamed CSV files saved to `../../data/cleaned/`",
   "id": "50b577d75aae631e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:21.196770Z",
     "start_time": "2025-11-20T20:08:50.213152Z"
    }
   },
   "cell_type": "code",
   "source": "dc.main()",
   "id": "8ec45af1c8067334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: ../../data/train/raw\n",
      "Cleaned: ../../data/train/cleaned\n",
      "\n",
      "Found 3 CSV files in ../../data/train/raw\n",
      "\n",
      "CSV Files List ⬇️\n",
      "  1. part03.csv (3.03 MB)\n",
      "  2. part02.csv (2.97 MB)\n",
      "  3. part01.csv (2.99 MB)\n",
      "\n",
      "============================================================\n",
      "Processing Files...\n",
      "============================================================\n",
      "\n",
      "[1/3] part03.csv... Amount:27362, Missing:1299, →10-05-2024_to_10-23-2024.csv\n",
      "[2/3] part02.csv... Amount:27361, Missing:1624, →09-19-2024_to_10-05-2024.csv\n",
      "[3/3] part01.csv... Amount:27351, Missing:1277, →09-01-2024_to_09-19-2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaned File Analysis",
   "id": "c372e04ff766b0fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Size & Rows & Date Span",
   "id": "3b312b770f289f51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:21.294037Z",
     "start_time": "2025-11-20T20:09:21.203401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "# Load all dataframes\n",
    "dfs = {}\n",
    "for filename in csv_files:\n",
    "    dfs[filename] = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "\n",
    "# Collect stats\n",
    "stats = []\n",
    "for filename, df in dfs.items():\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    min_date = df['Post Date'].min()\n",
    "    max_date = df['Post Date'].max()\n",
    "    fraud_count = ((df['Fraud Adjustment Indicator'].notna()) &\n",
    "                   (df['Fraud Adjustment Indicator'] != '')).sum()\n",
    "\n",
    "    stats.append({\n",
    "        'File': filename,\n",
    "        'Rows': len(df),\n",
    "        'Members': df['Member ID'].nunique(),\n",
    "        'Date From': min_date.strftime('%m/%d/%Y') if pd.notna(min_date) else 'N/A',\n",
    "        'Date To': max_date.strftime('%m/%d/%Y') if pd.notna(max_date) else 'N/A',\n",
    "        'Days': (max_date - min_date).days if pd.notna(min_date) else 0,\n",
    "        'Fraud %': round(fraud_count / len(df) * 100, 4) if len(df) > 0 else 0\n",
    "    })\n",
    "\n",
    "# Display table\n",
    "df_stats = pd.DataFrame(stats)\n",
    "display(df_stats)\n",
    "\n",
    "# Summary\n",
    "all_members = set()\n",
    "for df in dfs.values():\n",
    "    all_members.update(df['Member ID'].dropna())\n",
    "\n",
    "total_fraud = sum([((dfs[f]['Fraud Adjustment Indicator'].notna()) &\n",
    "                    (dfs[f]['Fraud Adjustment Indicator'] != '')).sum()\n",
    "                   for f in dfs.keys()])\n",
    "\n",
    "print(f\"\\nTotal Rows: {df_stats['Rows'].sum():,}\")\n",
    "print(f\"Total Unique Members: {len(all_members):,}\")\n",
    "print(f\"Total Fraud Indicators: {total_fraud:,}\")\n",
    "print(f\"Overall Fraud %: {round(total_fraud / df_stats['Rows'].sum() * 100, 4)}%\")"
   ],
   "id": "d9503066fa80edd1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           File   Rows  Members   Date From     Date To  Days  \\\n",
       "0  09-01-2024_to_09-19-2024.csv  27363     1116  09/01/2024  09/19/2024    18   \n",
       "1  09-19-2024_to_10-05-2024.csv  27363     1446  09/19/2024  10/05/2024    16   \n",
       "2  10-05-2024_to_10-23-2024.csv  27363     1129  10/05/2024  10/23/2024    18   \n",
       "\n",
       "   Fraud %  \n",
       "0   0.0365  \n",
       "1   0.0073  \n",
       "2   0.0731  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Members</th>\n",
       "      <th>Date From</th>\n",
       "      <th>Date To</th>\n",
       "      <th>Days</th>\n",
       "      <th>Fraud %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09-01-2024_to_09-19-2024.csv</td>\n",
       "      <td>27363</td>\n",
       "      <td>1116</td>\n",
       "      <td>09/01/2024</td>\n",
       "      <td>09/19/2024</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-19-2024_to_10-05-2024.csv</td>\n",
       "      <td>27363</td>\n",
       "      <td>1446</td>\n",
       "      <td>09/19/2024</td>\n",
       "      <td>10/05/2024</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-05-2024_to_10-23-2024.csv</td>\n",
       "      <td>27363</td>\n",
       "      <td>1129</td>\n",
       "      <td>10/05/2024</td>\n",
       "      <td>10/23/2024</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Rows: 82,089\n",
      "Total Unique Members: 1,490\n",
      "Total Fraud Indicators: 32\n",
      "Overall Fraud %: 0.039%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Overlapping detection",
   "id": "77b6792970472928"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:21.461484Z",
     "start_time": "2025-11-20T20:09:21.304972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CLEANED_DIR = dc.CLEANED_DIR\n",
    "csv_files = sorted([f for f in os.listdir(CLEANED_DIR) if f.endswith('.csv')])\n",
    "\n",
    "print(f\"Loading {len(csv_files)} files...\")\n",
    "\n",
    "# Load files and create row IDs\n",
    "file_rows = {}\n",
    "for filename in csv_files:\n",
    "    df = pd.read_csv(os.path.join(CLEANED_DIR, filename))\n",
    "    row_ids = set(df['Account ID'].astype(str) + '|' +\n",
    "                  df['Member ID'].astype(str) + '|' +\n",
    "                  df['Post Date'].astype(str) + '|' +\n",
    "                  df['Post Time'].astype(str) + '|' +\n",
    "                  df['Amount'].astype(str))\n",
    "    file_rows[filename] = row_ids\n",
    "\n",
    "# Calculate pairwise overlaps\n",
    "results = []\n",
    "for i, file1 in enumerate(csv_files):\n",
    "    for j, file2 in enumerate(csv_files):\n",
    "        if i < j:\n",
    "            overlap = len(file_rows[file1] & file_rows[file2])\n",
    "            pct1 = overlap / len(file_rows[file1]) * 100\n",
    "            pct2 = overlap / len(file_rows[file2]) * 100\n",
    "\n",
    "            results.append({\n",
    "                'File 1': file1,\n",
    "                'File 2': file2,\n",
    "                'Overlap Rows': overlap,\n",
    "                '% of File 1': round(pct1, 1),\n",
    "                '% of File 2': round(pct2, 1)\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Display as styled DataFrame\n",
    "display(df_results)"
   ],
   "id": "34a3032caa9d448c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3 files...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                         File 1                        File 2  Overlap Rows  \\\n",
       "0  09-01-2024_to_09-19-2024.csv  09-19-2024_to_10-05-2024.csv             1   \n",
       "1  09-01-2024_to_09-19-2024.csv  10-05-2024_to_10-23-2024.csv             0   \n",
       "2  09-19-2024_to_10-05-2024.csv  10-05-2024_to_10-23-2024.csv             0   \n",
       "\n",
       "   % of File 1  % of File 2  \n",
       "0          0.0          0.0  \n",
       "1          0.0          0.0  \n",
       "2          0.0          0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File 1</th>\n",
       "      <th>File 2</th>\n",
       "      <th>Overlap Rows</th>\n",
       "      <th>% of File 1</th>\n",
       "      <th>% of File 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09-01-2024_to_09-19-2024.csv</td>\n",
       "      <td>09-19-2024_to_10-05-2024.csv</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>09-01-2024_to_09-19-2024.csv</td>\n",
       "      <td>10-05-2024_to_10-23-2024.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>09-19-2024_to_10-05-2024.csv</td>\n",
       "      <td>10-05-2024_to_10-23-2024.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Feature Engineering: Description Encoding and Clustering\n\nThis stage performs advanced feature engineering on transaction descriptions:\n\n1. **BERT Encoding**: Use BERT-tiny model to encode \"Transaction Description\" text into embeddings\n2. **Dimensionality Reduction**: Apply PCA to reduce embedding dimensions (default: 20D)\n3. **Automatic Clustering**: Find optimal cluster count (k) via heuristic search and cluster with MiniBatchKMeans\n4. **Add Cluster ID**: Append `cluster_id` column to each CSV file\n\n**Input**: Cleaned CSV files from `../../data/cleaned/`  \n**Output**: Clustered files saved to `../../data/clustered_out/`\n\n**Note**: This step requires GPU/CPU compute and may take significant time depending on data size.",
   "id": "e268885f085a0a12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:37.066010Z",
     "start_time": "2025-11-20T20:09:21.485163Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = fe.main()",
   "id": "58b1d5fb190b6804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: DESCRIPTION ENCODING AND CLUSTERING\n",
      "============================================================\n",
      "Input: ../../data/train/cleaned\n",
      "Output: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/train/clustered_out\n",
      "Model: prajjwal1/bert-tiny\n",
      "Text Column: Transaction Description\n",
      "PCA Dimensions: 20\n",
      "Cluster Range: 10-60 (step 10)\n",
      "\n",
      "[Scan] Found 3 CSV file(s) in ../../data/train/cleaned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] Saved 3 clustered file(s) to /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/train/clustered_out\n",
      "\n",
      "============================================================\n",
      "STAGE 2 COMPLETE\n",
      "============================================================\n",
      "Processed 3 files\n",
      "Output location: /Users/wwy/Documents/CMU/25-Fall/Practicum/Clearshield/data/train/clustered_out\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Fraud Matching and Re-label\nThis cell performs fraud detection in two stages:\n\n1. **Reorganize by Member**: Group all transactions by Member ID into individual files (temp directory)\n2. **Match Fraud Adjustments**: Find and mark original fraudulent transactions for each refund record (≥10 transactions)\n   - Match by amount and date (extract from description or 30-day range)\n   - Prevent duplicate matching\n   - Categorize as matched/unmatched/no_fraud\n   - Automatically delete temp directory after processing\n\n**Input**: Clustered files from `../../data/train/clustered_out/`  \n**Output**: Processed member files saved to `../../data/train/by_member/[matched|unmatched|no_fraud]/`",
   "id": "64b755731928b893"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 3-1: Reorganize transactions by member\n\n**Input**: Clustered files from `../../data/train/clustered_out/`  \n**Output**: Member-grouped files saved to `../../data/train/by_member/temp/` (temporary)",
   "id": "f60bb8ed29d09f8e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:40.947552Z",
     "start_time": "2025-11-20T20:09:37.088932Z"
    }
   },
   "cell_type": "code",
   "source": "num_members = fr.run_stage1()",
   "id": "7156ce60494636e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1: DATA REORGANIZATION\n",
      "============================================================\n",
      "Input: ../../data/train/clustered_out\n",
      "Output: ../../data/train/by_member/temp\n",
      "\n",
      "Found 3 files\n",
      "Processing 1/3: 09-01-2024_to_09-19-2024.csv\n",
      "Processing 2/3: 09-19-2024_to_10-05-2024.csv\n",
      "Processing 3/3: 10-05-2024_to_10-23-2024.csv\n",
      "Modified 1490 member files this run\n",
      "Sorting modified files...\n",
      "  Sorted 1000/1490 files\n",
      "\n",
      "1490 member files created\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:40.992067Z",
     "start_time": "2025-11-20T20:09:40.973922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from glob import glob\n",
    "\n",
    "# Configuration\n",
    "BY_MEMBER_DIR = '../../data/by_member'\n",
    "n = 10  # Threshold\n",
    "\n",
    "# Get all member files and count transactions\n",
    "member_files = glob(os.path.join(BY_MEMBER_DIR, 'member_*.csv'))\n",
    "counts = [len(pd.read_csv(f)) for f in member_files]\n",
    "\n",
    "# Calculate statistics\n",
    "total_count = len(counts)\n",
    "above_n = sum(1 for c in counts if c >= n)\n",
    "below_n = total_count - above_n\n",
    "above_ratio = (above_n / total_count) * 100\n",
    "below_ratio = (below_n / total_count) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Threshold set to: {n}\")\n",
    "print(f\"Records >= {n}: {above_n:,} ({above_ratio:.2f}%)\")\n",
    "print(f\"Records < {n}: {below_n:,} ({below_ratio:.2f}%)\")"
   ],
   "id": "5f78f9336e45ffef",
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m above_n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m counts \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m n)\n\u001B[1;32m     14\u001B[0m below_n \u001B[38;5;241m=\u001B[39m total_count \u001B[38;5;241m-\u001B[39m above_n\n\u001B[0;32m---> 15\u001B[0m above_ratio \u001B[38;5;241m=\u001B[39m (\u001B[43mabove_n\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtotal_count\u001B[49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     16\u001B[0m below_ratio \u001B[38;5;241m=\u001B[39m (below_n \u001B[38;5;241m/\u001B[39m total_count) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stage 3-2: Fraud detection and matching\n\nFilter members with minimum history length (≥10 transactions), then match fraud adjustments to original transactions.\n\n**Input**: Member-grouped files from `../../data/train/by_member/temp/`  \n**Output**: Processed and categorized files saved to `../../data/train/by_member/[matched|unmatched|no_fraud]/`\n\n**Note**: The temp directory will be automatically deleted after processing completes.",
   "id": "ec2b1ea69aab8d16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:40.993216Z",
     "start_time": "2025-11-20T05:22:17.041743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Stage 2: Fraud detection with minimum history length filter\n",
    "min_history_length = 10\n",
    "stats = fr.run_stage2(min_history_length)"
   ],
   "id": "cb6b733ec61d5e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 2: FRAUD DETECTION\n",
      "============================================================\n",
      "Input: ../../data/train/by_member/temp\n",
      "Output: ../../data/train/by_member\n",
      "Min History Length: 10\n",
      "\n",
      "Found 0 member files\n",
      "Filtering: only processing members with >= 10 transactions\n",
      "Summary saved to: ../../data/train/by_member/member_summary.csv\n",
      "\n",
      "Processing Summary:\n",
      "  Total Processed: 0\n",
      "  Skipped (< 10 txns): 0\n",
      "  No Fraud: 0\n",
      "  Matched: 0\n",
      "  Unmatched: 0\n",
      "\n",
      "============================================================\n",
      "COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "ou5w7jmthd8",
   "source": "## 4. Feature Encoding\n\nThis stage encodes categorical features and prepares the final dataset for model training:\n\n1. **Remove ID Columns**: Delete Account ID and Member ID\n2. **Encode Categorical Features**: Convert categorical columns to numeric using predefined dictionary\n   - Account Type, Action Type, Source Type, Product ID\n3. **Parse Time Features**: Convert Post Time to decimal hours\n4. **Convert Date Features**: Parse Post Date and Account Open Date\n5. **Clean Up**: Remove text columns (Transaction Description, Fraud Adjustment Indicator)\n\n**Input**: Processed member files from `../../data/train/by_member/[matched|unmatched|no_fraud]/`  \n**Output**: Final encoded files saved to `../../data/train/final/[matched|unmatched|no_fraud]/`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cmq993u1rh",
   "source": "total_processed = enc.encode_features(enc.PROCESSED_DIR, enc.OUTPUT_DIR, enc.CONFIG_PATH)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:09:40.993443Z",
     "start_time": "2025-11-19T22:15:41.069840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENCODING\n",
      "============================================================\n",
      "Input Dir: ../../data/train/processed\n",
      "Output Dir: ../../data/train/final\n",
      "Config Path: ../../config/tokenize_dict.json\n",
      "\n",
      "Loaded encoding dictionary with 4 features\n",
      "\n",
      "matched: Found 96 files\n",
      "  matched: Encoded 96/96 files\n",
      "\n",
      "unmatched: Found 17 files\n",
      "  unmatched: Encoded 17/17 files\n",
      "\n",
      "no_fraud: Found 1391 files\n",
      "  Processed 100/1391 files\n",
      "  Processed 200/1391 files\n",
      "  Processed 300/1391 files\n",
      "  Processed 400/1391 files\n",
      "  Processed 500/1391 files\n",
      "  Processed 600/1391 files\n",
      "  Processed 700/1391 files\n",
      "  Processed 800/1391 files\n",
      "  Processed 900/1391 files\n",
      "  Processed 1000/1391 files\n",
      "  Processed 1100/1391 files\n",
      "  Processed 1200/1391 files\n",
      "  Processed 1300/1391 files\n",
      "  no_fraud: Encoded 1391/1391 files\n",
      "\n",
      "============================================================\n",
      "Encoding Complete!\n",
      "Total files found: 1504\n",
      "Total files processed: 1504\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "75bd63344823637a",
   "source": "TODO : vulnerability scanner",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n\n## Pipeline Complete!\n\nThe complete data preprocessing pipeline consists of 4 stages:\n\n1. **Data Cleaning**: Raw CSV → Cleaned CSV (`data/train/cleaned/`)\n2. **Feature Engineering**: Cleaned CSV → Clustered CSV (`data/train/clustered_out/`)\n3. **Fraud Matching**: Clustered CSV → Categorized by Member (`data/train/by_member/[matched|unmatched|no_fraud]/`)\n4. **Feature Encoding**: Processed CSV → Final Encoded Dataset (`data/train/final/[matched|unmatched|no_fraud]/`)\n\n**Final Output**: `../../data/train/final/[matched|unmatched|no_fraud]/member_*.csv`\n\nThese final encoded files are ready for model training!",
   "id": "5o3ixzv5ahs"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
