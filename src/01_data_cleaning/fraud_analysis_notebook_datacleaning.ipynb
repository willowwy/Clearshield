{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab96efadfc36ada",
   "metadata": {},
   "source": [
    "# Fraud Transaction Analysis\n",
    "Analysis of fraud patterns in transaction data with key visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c4ae5df2b2618",
   "metadata": {},
   "source": [
    " ## Load and Prepare Data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"Load and prepare transaction 01_data_cleaning for analysis\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert amount column to numeric\n",
    "    df['Amount'] = df['Amount'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "    # Convert dates\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    df['Account Open Date'] = pd.to_datetime(df['Account Open Date'], errors='coerce')\n",
    "\n",
    "    # Create fraud indicator\n",
    "    df['Is_Fraud'] = df['Fraud Adjustment Indicator'].notna() & (df['Fraud Adjustment Indicator'].str.strip() != '')\n",
    "\n",
    "    # Clean IDs\n",
    "    df['Member ID'] = df['Member ID'].astype(str).str.lstrip('0')\n",
    "    df['Account ID'] = df['Account ID'].astype(str).str.lstrip('0')\n",
    "\n",
    "    # Basic statistics\n",
    "    total_transactions = len(df)\n",
    "    fraud_cases = df['Is_Fraud'].sum()\n",
    "    fraud_rate = fraud_cases / total_transactions * 100\n",
    "\n",
    "    print(f\"Dataset Overview:\")\n",
    "    print(f\"- Total transactions: {total_transactions:,}\")\n",
    "    print(f\"- Fraud cases: {fraud_cases:,}\")\n",
    "    print(f\"- Fraud rate: {fraud_rate:.4f}%\")\n",
    "    print(f\"- Date range: {df['Post Date'].min().strftime('%Y-%m-%d')} to {df['Post Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"- Unique accounts: {df['Account ID'].nunique():,}\")\n",
    "    print(f\"- Unique members: {df['Member ID'].nunique():,}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load 01_data_cleaning\n",
    "file_path_1 = '../../data/raw/transaction_data.csv'\n",
    "file_path_2 = '../../data/raw/Transaction Data 09232025.csv'\n",
    "df1 = load_and_prepare_data(file_path_1)\n",
    "df2 = load_and_prepare_data(file_path_2)\n",
    "\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "\n",
    "output_path = \"/processed/transaction_data_merged.csv\"\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {df_combined.shape}\")\n",
    "display(df_combined.head())"
   ],
   "id": "b3f3bf77ea9c84fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "merged_file = \"/processed/transaction_data_merged.csv\"\n",
    "cleaned_file = \"../../data/processed/transaction_data_cleaned.csv\"\n",
    "\n",
    "# Read Data\n",
    "df = pd.read_csv(merged_file)\n",
    "\n",
    "missing_tbl = (\n",
    "    df.isna().sum().rename('missing_count').to_frame()\n",
    ")\n",
    "missing_tbl['missing_pct'] = (missing_tbl['missing_count'] / len(df) * 100).round(2)\n",
    "print(\"\\nMissing overview BEFORE cleaning:\")\n",
    "print(\n",
    "    missing_tbl[missing_tbl['missing_count'] > 0]\n",
    "      .sort_values('missing_count', ascending=False)\n",
    "      .head(30)\n",
    ")\n",
    "\n",
    "\n",
    "# Amount Missing\n",
    "df['Amount_missing'] = df['Amount'].isna()\n",
    "amount_mean = df['Amount'].mean(skipna=True)\n",
    "df['Amount'] = df['Amount'].fillna(amount_mean)\n",
    "\n",
    "# Type Missing\n",
    "for col in ['Type', 'Product ID', 'Action Type', 'Transaction Description']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# Age == 0 \n",
    "if 'Member Age' in df.columns:\n",
    "    age_zero_count = (df['Member Age'] == 0).sum()\n",
    "    print(f\"Age==0 numbers: {age_zero_count}\")\n",
    "\n",
    "    # Fix by mean\n",
    "    age_median = df.loc[df['Member Age'] > 0, 'Member Age'].median()\n",
    "    df.loc[df['Member Age'] == 0, 'Member Age'] = age_median\n",
    "\n",
    "# Outliers\n",
    "Q1 = df['Amount'].quantile(0.25)\n",
    "Q3 = df['Amount'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "def filter_outliers(row):\n",
    "    if row['Is_Fraud']:  # fraud 01_data_cleaning keep\n",
    "        return True\n",
    "    # common 01_data_cleaning ‚Üí check if in the field\n",
    "    return (lower_bound <= row['Amount'] <= upper_bound)\n",
    "\n",
    "df = df[df.apply(filter_outliers, axis=1)]\n",
    "\n",
    "\n",
    "df.to_csv(cleaned_file, index=False)\n",
    "print(f\"Cleaned 01_data_cleaning saved to: {cleaned_file}\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "display(df.head())\n"
   ],
   "id": "cf5bae698547ae12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n=== Data Cleaning Validation ===\")\n",
    "\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "if 'Member Age' in df.columns:\n",
    "    print(f\"Remaining Age==0 count: {(df['Member Age'] == 0).sum()}\")\n",
    "\n",
    "df_raw = pd.read_csv(merged_file)\n",
    "fraud_removed = df_raw[df_raw['Is_Fraud'] & ~df_raw.index.isin(df.index)]\n",
    "print(f\"Fraud rows removed during cleaning: {len(fraud_removed)}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "df_raw['Amount'].hist(alpha=0.5, bins=50, label='Raw')\n",
    "df['Amount'].hist(alpha=0.5, bins=50, label='Cleaned')\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Amount Distribution Before vs After Cleaning\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "385999ff8728ddc3"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca943cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/transaction_data_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m raw \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmerged_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[1;32m      7\u001B[0m missing_counts \u001B[38;5;241m=\u001B[39m raw\u001B[38;5;241m.\u001B[39misna()\u001B[38;5;241m.\u001B[39msum()\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/Documents/CMU/25-Fall/Practicum/Clearshield/.venv/lib/python3.9/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/raw/transaction_data_merged.csv'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_csv(merged_file)\n",
    "\n",
    "# \n",
    "missing_counts = raw.isna().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "total_rows = len(raw)\n",
    "missing_pct = (missing_counts / total_rows * 100)\n",
    "\n",
    "#\n",
    "plt.figure(figsize=(10, max(4, 0.35*len(missing_pct))))\n",
    "plt.barh(missing_pct.index[::-1], missing_pct.values[::-1])\n",
    "for y, v in enumerate(missing_pct.values[::-1]):\n",
    "    plt.text(v + 0.2, y, f'{v:.2f}%', va='center')\n",
    "plt.xlabel('Missing percentage (%)')\n",
    "plt.title('Missing values per column (all)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# \n",
    "pie_series = missing_pct.drop('Fraud Adjustment Indicator', errors='ignore')\n",
    "\n",
    "if not pie_series.empty:\n",
    "    # combine 1%\n",
    "    small_mask = pie_series < 1\n",
    "    other_sum = pie_series[small_mask].sum()\n",
    "    pie_series = pie_series[~small_mask]\n",
    "    if other_sum > 0:\n",
    "        pie_series.loc['Other (<1%)'] = other_sum\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    wedges, _, autotexts = plt.pie(\n",
    "        pie_series.values,\n",
    "        labels=None,      \n",
    "        startangle=90,\n",
    "        autopct=lambda p: f'{p:.2f}%' if p >= 1 else ''\n",
    "    )\n",
    "    plt.legend(\n",
    "        wedges,\n",
    "        [f'{k}' for k in pie_series.index],\n",
    "        title='Columns',\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.0, 0.5)\n",
    "    )\n",
    "    plt.title('Missing value breakdown (excluding dominant column)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No columns to plot in the pie chart after excluding the dominant column.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compact Fraud Matching Algorithm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import timedelta\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def extract_date_merchant(description, adj_date):\n",
    "    \"\"\"Extract date and merchant information from fraud adjustment description\"\"\"\n",
    "    if pd.isna(description): return None, None\n",
    "    desc = str(description).upper()\n",
    "    \n",
    "    # Extract date pattern\n",
    "    date_match = re.search(r'(\\d{1,2}/\\d{1,2})', desc)\n",
    "    extracted_date = None\n",
    "    if date_match:\n",
    "        try:\n",
    "            month, day = map(int, date_match.group(1).split('/'))\n",
    "            year = adj_date.year - 1 if (month > adj_date.month or \n",
    "                (month == adj_date.month and day > adj_date.day)) else adj_date.year\n",
    "            extracted_date = pd.Timestamp(year, month, day)\n",
    "        except: pass\n",
    "    \n",
    "    # Extract merchant name\n",
    "    clean_desc = re.sub(r'^(RTN|RETURN|UNAUTH|TRANS).*?|(RETURN|UNAUTH|TRANS).*$|\\d{1,2}/\\d{1,2}', '', desc).strip()\n",
    "    merchant = clean_desc if clean_desc and len(clean_desc) > 2 else None\n",
    "    \n",
    "    return extracted_date, merchant\n",
    "\n",
    "def similarity(a, b):\n",
    "    \"\"\"Calculate string similarity score\"\"\"\n",
    "    if pd.isna(a) or pd.isna(b): return 0\n",
    "    return SequenceMatcher(None, str(a).upper(), str(b).upper()).ratio()\n",
    "\n",
    "def get_candidates(adj, txns, used_idx, date_filter=None, amount_filter='exact'):\n",
    "    \"\"\"Get candidate transactions based on filters\"\"\"\n",
    "    fraud_amount, fraud_date = abs(adj['Amount']), adj['Post Date']\n",
    "    filters = [~txns.index.isin(used_idx)]\n",
    "    \n",
    "    # Amount filter\n",
    "    if amount_filter == 'window':\n",
    "        start_date = fraud_date - timedelta(days=30)\n",
    "        filters.extend([txns['Post Date'] >= start_date, txns['Post Date'] <= fraud_date])\n",
    "    elif amount_filter == 'before':\n",
    "        filters.append(txns['Post Date'] <= fraud_date)\n",
    "    filters.append(abs(txns['Amount']) == fraud_amount)\n",
    "    \n",
    "    # Date filter\n",
    "    if isinstance(date_filter, pd.Timestamp):\n",
    "        filters.append(txns['Post Date'] == date_filter)\n",
    "    elif date_filter == 'before':\n",
    "        filters.append(txns['Post Date'] < fraud_date)\n",
    "    elif date_filter == 'same_day_before':\n",
    "        filters.extend([txns['Post Date'] == fraud_date, txns['Post Time'] < adj['Post Time']])\n",
    "    \n",
    "    return txns[np.logical_and.reduce(filters)]\n",
    "\n",
    "def match_fraud_to_transaction(adj, txns, used_idx):\n",
    "    \"\"\"Multi-level fraud matching algorithm\"\"\"\n",
    "    # Level 1: Exact match (Date + Amount + Merchant)\n",
    "    extracted_date, merchant = extract_date_merchant(adj['Fraud Adjustment Indicator'], adj['Post Date'])\n",
    "    if extracted_date and merchant:\n",
    "        candidates = get_candidates(adj, txns, used_idx, extracted_date)\n",
    "        if not candidates.empty:\n",
    "            candidates['similarity'] = candidates['Transaction Description'].apply(lambda x: similarity(x, merchant))\n",
    "            best = candidates.loc[candidates['similarity'].idxmax()]\n",
    "            if best['similarity'] > 0.6:\n",
    "                return best.name, 'exact_match'\n",
    "    \n",
    "    # Level 2: Date + Amount match\n",
    "    if extracted_date:\n",
    "        candidates = get_candidates(adj, txns, used_idx, extracted_date)\n",
    "        if not candidates.empty:\n",
    "            return candidates.iloc[0].name, 'date_amount_match'\n",
    "    \n",
    "    # Level 3: Time window + Amount match\n",
    "    before_candidates = get_candidates(adj, txns, used_idx, 'before')\n",
    "    if not before_candidates.empty:\n",
    "        return before_candidates.loc[before_candidates['Post Date'].idxmax()].name, 'time_window_match'\n",
    "    \n",
    "    # Same day earlier time\n",
    "    if 'Post Time' in txns.columns:\n",
    "        same_day = get_candidates(adj, txns, used_idx, 'same_day_before')\n",
    "        if not same_day.empty:\n",
    "            return same_day.loc[same_day['Post Time'].idxmax()].name, 'time_window_match'\n",
    "    \n",
    "    # Level 4: Amount only match\n",
    "    candidates = get_candidates(adj, txns, used_idx, amount_filter='before')\n",
    "    if not candidates.empty:\n",
    "        candidates['days_diff'] = (candidates['Post Date'].max() - candidates['Post Date']).dt.days\n",
    "        return candidates.loc[candidates['days_diff'].idxmin()].name, 'amount_only_match'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_member_fraud(member_data, member_id, output_dir):\n",
    "    \"\"\"Process fraud matching for a single member\"\"\"\n",
    "    df = member_data.copy()\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'])\n",
    "    df['Is_Fraud'] = 0\n",
    "    \n",
    "    # Sort by time\n",
    "    sort_cols = ['Post Date', 'Post Time'] if 'Post Time' in df.columns else ['Post Date']\n",
    "    df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "    \n",
    "    # Find fraud adjustment records\n",
    "    fraud_adjustments = df[df['Fraud Adjustment Indicator'].notna() & \n",
    "                          (df['Fraud Adjustment Indicator'].str.strip() != '')]\n",
    "    \n",
    "    # Initialize return values\n",
    "    matched_count = 0\n",
    "    total_adjustments = len(fraud_adjustments)\n",
    "    \n",
    "    if fraud_adjustments.empty:\n",
    "        category = 'no_fraud'\n",
    "    else:\n",
    "        # Preprocessing checks\n",
    "        if len(df) == 1 and not fraud_adjustments.empty:\n",
    "            category = 'no_fraud'  # Single transaction that is an adjustment record\n",
    "        else:\n",
    "            # Check if all adjustments have no matching amounts\n",
    "            no_match_count = 0\n",
    "            for _, adj in fraud_adjustments.iterrows():\n",
    "                fraud_amount = abs(adj['Amount'])\n",
    "                other_txns = df[df.index != adj.name]\n",
    "                if not (abs(other_txns['Amount']) == fraud_amount).any():\n",
    "                    no_match_count += 1\n",
    "            \n",
    "            if no_match_count == len(fraud_adjustments):\n",
    "                category = 'no_fraud'  # All adjustments have no matching amounts\n",
    "            else:\n",
    "                # Execute matching\n",
    "                used_indices = set()\n",
    "                successful_adjustments = []  # Track successfully matched adjustments\n",
    "                \n",
    "                for _, fraud_adj in fraud_adjustments.iterrows():\n",
    "                    fraud_amount = abs(fraud_adj['Amount'])\n",
    "                    other_txns = df[df.index != fraud_adj.name]\n",
    "                    \n",
    "                    # Skip adjustment if no matching amount exists\n",
    "                    if not (abs(other_txns['Amount']) == fraud_amount).any():\n",
    "                        continue\n",
    "                    \n",
    "                    match_result = match_fraud_to_transaction(fraud_adj, df, used_indices)\n",
    "                    if match_result:\n",
    "                        original_idx, match_type = match_result\n",
    "                        df.loc[original_idx, 'Is_Fraud'] = 1\n",
    "                        used_indices.add(original_idx)\n",
    "                        matched_count += 1\n",
    "                        successful_adjustments.append(fraud_adj.name)\n",
    "                \n",
    "                # Recalculate valid adjustments count (excluding those with no matching amounts)\n",
    "                valid_adjustments = 0\n",
    "                for _, adj in fraud_adjustments.iterrows():\n",
    "                    fraud_amount = abs(adj['Amount'])\n",
    "                    other_txns = df[df.index != adj.name]\n",
    "                    if (abs(other_txns['Amount']) == fraud_amount).any():\n",
    "                        valid_adjustments += 1\n",
    "                \n",
    "                # Determine category based on valid adjustments count\n",
    "                if matched_count == valid_adjustments and matched_count > 0:\n",
    "                    category = 'matched'\n",
    "                elif matched_count > 0:\n",
    "                    category = 'unmatched'\n",
    "                else:\n",
    "                    category = 'problematic'\n",
    "                \n",
    "                # Update total_adjustments to valid adjustments count\n",
    "                total_adjustments = valid_adjustments\n",
    "    \n",
    "    # Save file\n",
    "    subfolder_map = {'matched': 'fully_matched', 'unmatched': 'partially_matched', \n",
    "                     'problematic': 'problematic_cases', 'no_fraud': 'unmatched_cases'}\n",
    "    category_dir = os.path.join(output_dir, subfolder_map[category])\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare output 01_data_cleaning with standardized column order\n",
    "    output_columns = ['Account ID', 'Member ID', 'Account Type', 'Account Open Date', \n",
    "                     'Member Age', 'Product ID', 'Post Date', 'Post Time', 'Amount', \n",
    "                     'Action Type', 'Source Type', 'Transaction Description', \n",
    "                     'Fraud Adjustment Indicator', 'Is_Fraud']\n",
    "    \n",
    "    # Keep all records, do not remove any rows\n",
    "    # Fraud Adjustment Indicator values remain unchanged\n",
    "    output_data = df.copy()\n",
    "    \n",
    "    # Ensure all required columns exist and arrange in specified order\n",
    "    for col in output_columns:\n",
    "        if col not in output_data.columns:\n",
    "            if col == 'Is_Fraud':\n",
    "                output_data[col] = 0\n",
    "            else:\n",
    "                output_data[col] = None\n",
    "    \n",
    "    output_data = output_data[output_columns]\n",
    "    \n",
    "    filename = f\"member_{member_id}.csv\"\n",
    "    output_path = os.path.join(category_dir, filename)\n",
    "    output_data.to_csv(output_path, index=False)\n",
    "    \n",
    "    return category, matched_count, total_adjustments\n",
    "\n",
    "def process_fraud_data_complete(input_csv_path, output_dir='../01_data_cleaning/processed'):\n",
    "    \"\"\"Complete fraud 01_data_cleaning processing pipeline\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading transaction 01_data_cleaning...\")\n",
    "    df = pd.read_csv(input_csv_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df):,} transactions\")\n",
    "    \n",
    "    # Identify only fraud victim members (members with fraud indicators)\n",
    "    fraud_victims = df[df['Fraud Adjustment Indicator'].notna() & \n",
    "                      (df['Fraud Adjustment Indicator'].str.strip() != '')]['Member ID'].unique()\n",
    "    \n",
    "    print(f\"Found {len(fraud_victims)} fraud victim members\")\n",
    "    print(\"Note: Only members with fraud indicators will be processed and output\")\n",
    "    \n",
    "    # Processing statistics\n",
    "    counts = {'matched': 0, 'unmatched': 0, 'problematic': 0, 'no_fraud': 0}\n",
    "    summary_results = []\n",
    "    \n",
    "    # Process only fraud victims\n",
    "    print(\"\\nProcessing fraud victims...\")\n",
    "    for i, member_id in enumerate(fraud_victims, 1):\n",
    "        try:\n",
    "            print(f\"Processing member {member_id} ({i}/{len(fraud_victims)})\")\n",
    "            member_data = df[df['Member ID'] == member_id]\n",
    "            category, matches_found, total_adjustments = process_member_fraud(member_data, member_id, output_dir)\n",
    "            \n",
    "            counts[category] += 1\n",
    "            summary_results.append({\n",
    "                'Member_ID': member_id, 'Category': category, 'Fraud_Found': matches_found,\n",
    "                'Total_Adjustments': total_adjustments, 'Match_Rate': f\"{matches_found}/{total_adjustments}\",\n",
    "                'All_Matched': matches_found == total_adjustments and matches_found > 0\n",
    "            })\n",
    "            \n",
    "            print(f\"  Result: {matches_found}/{total_adjustments} -> {category}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            counts['problematic'] += 1\n",
    "    \n",
    "    # Save summary report\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    summary_df.to_csv(os.path.join(output_dir, 'fraud_processing_summary.csv'), index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Fraud 01_data_cleaning processing completed!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total fraud victim members processed: {len(fraud_victims):,}\")\n",
    "    print(f\"Fully matched: {counts['matched']:,}\")\n",
    "    print(f\"Partially matched: {counts['unmatched']:,}\")\n",
    "    print(f\"Problematic: {counts['problematic']:,}\")\n",
    "    print(f\"Unmatched cases (after processing): {counts['no_fraud']:,}\")\n",
    "    print(f\"\\nResults saved to: {output_dir}\")\n",
    "    \n",
    "    return len(fraud_victims), counts\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting fraud 01_data_cleaning processing...\")\n",
    "    input_file = '../../data/processed/transaction_data_cleaned.csv'\n",
    "    output_dir = '../../data/processed'\n",
    "    \n",
    "    total_members, counts = process_fraud_data_complete(input_file, output_dir)\n",
    "    \n",
    "    print(f\"\\nProcessing completed! Total members: {total_members:,}\")\n",
    "    print(\"üìÅ Output folders: fully_matched, partially_matched, problematic_cases, unmatched_cases\")\n",
    "\n",
    "print(\"Compact fraud matching algorithm loaded!\")"
   ],
   "id": "95754593244352d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
