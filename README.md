# ğŸ” Clearshield

A machine learning system for detecting fraudulent transactions using traditional ML algorithms and LSTM neural networks.

## ğŸ“‹ Overview

This fraud detection system combines traditional machine learning with deep learning approaches to identify fraudulent transactions. The system features a hybrid architecture that leverages both statistical features and sequential patterns.

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone <repository-url>
cd ClearShield

# Setup (create directories + install dependencies)
make setup
```

Or manually:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
python setup.py
```

### 2. Data Preprocessing

Place your raw data in `data/train/raw/` or `data/pred/raw/`, then run:

```bash
# Training data pipeline (5 stages)
make run-train

# Prediction data pipeline (4 stages)
make run-pred
```

Or use Python directly:
```bash
cd src/data_preprocess
python run_train_pipeline.py          # Training mode
python run_pred_pipeline.py           # Prediction mode
```

### 3. Model Training

```bash
# Train sequence model (Step 1)
make train-seq

# Train judge model (Step 2)
make train-judge

# Or train both models sequentially
make train-all
```

Advanced options:
```bash
# Custom training parameters
make train-seq EPOCHS=200 MAX_LEN=100 SAVE_DIR=my_checkpoints
```

### 4. Model Inference

```bash
# Run inference on a single member
make infer MEMBER_ID=12345

# Use custom data folder
make infer MEMBER_ID=12345 INFER_FOLDER=data/pred/final
```

Or use Python directly:
```bash
cd src/models
python inference.py \
  --folder ../../data/train/final/matched \
  --member_id 12345 \
  --sequence_model_path ../../checkpoints/best_model_enc.pth \
  --judge_model_path ../../checkpoints/best_judge_model.pth \
  --max_len 50
```

### 5. View All Commands

```bash
make help
```

## ğŸ”„ Data Processing Pipeline

The system processes data through 5 sequential stages:

### Training Pipeline (5 Stages)
1. **Data Cleaning** - Standardize headers, fix formatting, clean Amount field
2. **Feature Engineering** - BERT encoding, PCA reduction, clustering (k=60)
3. **Fraud Matching** - Reorganize by member, match fraud adjustments, categorize
4. **Feature Encoding** - Encode categorical features, parse time/date fields
5. **Vulnerability Scanning** (Optional) - Security testing, adversarial attack detection

**Data Flow**: `raw/` â†’ `cleaned/` â†’ `clustered_out/` â†’ `by_member/` â†’ `final/`

### Prediction Pipeline (4 Stages)
Same as training pipeline but uses pre-trained clustering models (Stages 1-4 only).

**Output**: Model-ready datasets in `data/train/final/` or `data/pred/final/`

## ğŸ”§ Configuration

### Pipeline Parameters

**Training Pipeline:**
```bash
python run_train_pipeline.py --help                    # Show all options
python run_train_pipeline.py --min-history 15          # Minimum 15 transactions per member
python run_train_pipeline.py --skip-vuln-scan          # Skip security scanning
python run_train_pipeline.py --vuln-sample-size 2000   # Use 2000 samples for scanning
```

**Model Training:**
```bash
# Configurable via Makefile variables
EPOCHS=110              # Training epochs (default: 110)
MAX_LEN=50              # Maximum sequence length (default: 50)
SAVE_DIR=checkpoints    # Model save directory (default: checkpoints)
```

### Custom Data Paths

Override paths programmatically:
```python
from src.data_preprocess.feature_engineering import run_stage2

run_stage2(
    processed_dir='/custom/path/to/cleaned',
    output_dir='/custom/path/to/output',
    model_name='prajjwal1/bert-tiny',
    pca_dim=20,
    max_k=60
)
```

## ğŸ”® Inference Mode

For processing new data using pre-trained models:

### Using Prediction Pipeline
```bash
# Place new data in data/pred/raw/
make run-pred

# Run inference
make infer MEMBER_ID=12345 INFER_FOLDER=data/pred/final
```

### Manual Stage 2 Inference
```bash
cd src/data_preprocess/02_feature_engineering
python inference_stage2.py \
  --input ../../data/pred/cleaned \
  --output ../../data/pred/clustered_out \
  --model cluster_model.pkl
```

## ğŸ§¹ Cleaning

```bash
make clean-train      # Clean training data
make clean-pred       # Clean prediction data
make clean-models     # Remove model checkpoints
make clean            # Clean all data directories
```

## ğŸ“ Project Structure

```
ClearShield/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                   # Training data pipeline
â”‚   â”‚   â”œâ”€â”€ raw/                 # Original datasets
â”‚   â”‚   â”œâ”€â”€ cleaned/             # Cleaned datasets (Stage 1)
â”‚   â”‚   â”œâ”€â”€ clustered_out/       # Clustered datasets (Stage 2)
â”‚   â”‚   â”œâ”€â”€ by_member/           # Fraud-matched datasets (Stage 3)
â”‚   â”‚   â”‚   â”œâ”€â”€ temp/            # Temporary files
â”‚   â”‚   â”‚   â”œâ”€â”€ matched/         # Fraud matched members
â”‚   â”‚   â”‚   â”œâ”€â”€ unmatched/       # Fraud unmatched members
â”‚   â”‚   â”‚   â””â”€â”€ no_fraud/        # No fraud members
â”‚   â”‚   â””â”€â”€ final/               # Final encoded datasets (Stage 4)
â”‚   â”‚       â”œâ”€â”€ matched/         # Model-ready
â”‚   â”‚       â”œâ”€â”€ unmatched/
â”‚   â”‚       â””â”€â”€ no_fraud/
â”‚   â”‚
â”‚   â””â”€â”€ pred/                    # Prediction data pipeline
â”‚       â”œâ”€â”€ raw/                 # New transaction data
â”‚       â”œâ”€â”€ cleaned/
â”‚       â”œâ”€â”€ clustered_out/
â”‚       â”œâ”€â”€ by_member/
â”‚       â””â”€â”€ final/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocess/
â”‚   â”‚   â”œâ”€â”€ 01_data_cleaning/
â”‚   â”‚   â”œâ”€â”€ 02_feature_engineering/
â”‚   â”‚   â”‚   â”œâ”€â”€ 02a_transaction_type_clustering/
â”‚   â”‚   â”‚   â””â”€â”€ 02b_description_encoding/
â”‚   â”‚   â”œâ”€â”€ 03_fraud_relabeling/
â”‚   â”‚   â”œâ”€â”€ 04_encoding/
â”‚   â”‚   â”œâ”€â”€ 05_security/
â”‚   â”‚   â”œâ”€â”€ train_pipeline.ipynb
â”‚   â”‚   â”œâ”€â”€ pred_pipeline.ipynb
â”‚   â”‚   â”œâ”€â”€ run_train_pipeline.py
â”‚   â”‚   â””â”€â”€ run_pred_pipeline.py
â”‚   â”‚
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ backbone_model.py    # Sequence model
â”‚       â”œâ”€â”€ judge.py             # Fraud judge model
â”‚       â”œâ”€â”€ datasets.py          # Data loaders
â”‚       â”œâ”€â”€ inference.py         # Inference script
â”‚       â”œâ”€â”€ load_model.py        # Model loading utilities
â”‚       â””â”€â”€ loss.py              # Loss functions
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ pipeline_config.py       # Centralized path configuration
â”‚   â””â”€â”€ tokenize_dict.json       # Categorical encoding dictionary
â”‚
â”œâ”€â”€ checkpoints/                 # Trained model checkpoints
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”œâ”€â”€ docs/                        # Documentation
â”‚
â”œâ”€â”€ train.py                     # Sequence model training
â”œâ”€â”€ train_judge.py               # Judge model training
â”œâ”€â”€ Makefile                     # Automation commands
â”œâ”€â”€ setup.py                     # Project setup script
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md
```

## ğŸ“¦ Dependencies

See `requirements.txt` for full list. Key dependencies:
- PyTorch â‰¥2.2.0
- Transformers â‰¥4.38.0
- scikit-learn â‰¥1.3.0
- pandas â‰¥2.0.0
- numpy â‰¥1.24.0
- joblib â‰¥1.3.0
- cryptography â‰¥41.0.0

## ğŸ” Security

Stage 5 vulnerability scanning includes:
- Data poisoning detection
- Adversarial attack testing (FGSM, PGD)
- Privacy attack simulation
- Automated security reporting

Output: `vulnerability_scan_results.json`
